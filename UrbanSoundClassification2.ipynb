{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6accb5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a85d7b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relative_path</th>\n",
       "      <th>classID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/road/Around Arya School(1Â°16_32_ S 36Â°49_29_ ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/road/Around Arya School(1Â°16_32_ S 36Â°49_29_ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/road/Around Arya School(1Â°16_32_ S 36Â°49_29_ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/road/Around Arya School(1Â°16_32_ S 36Â°49_29_ ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/road/Around Arya School(1Â°16_32_ S 36Â°49_29_ ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       relative_path  classID\n",
       "0  /road/Around Arya School(1Â°16_32_ S 36Â°49_29_ ...        5\n",
       "1  /road/Around Arya School(1Â°16_32_ S 36Â°49_29_ ...        1\n",
       "2  /road/Around Arya School(1Â°16_32_ S 36Â°49_29_ ...        1\n",
       "3  /road/Around Arya School(1Â°16_32_ S 36Â°49_29_ ...        6\n",
       "4  /road/Around Arya School(1Â°16_32_ S 36Â°49_29_ ...        9"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Prepare training data from Metadata file\n",
    "# ----------------------------\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "download_path  = Path('data\\\\DATA\\\\output_chunks')\n",
    "# Read metadata file\n",
    "metadata_file = download_path/'metadata.csv'\n",
    "df = pd.read_csv(metadata_file)\n",
    "df.head()\n",
    "# Construct file path by concatenating road and slice_file_name\n",
    "df['relative_path'] = '/road/' + df['road'].astype(str) + '/' + df['slice_file_name'].astype(str)\n",
    "# Take relevant columns\n",
    "df = df[['relative_path', 'classID']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c509e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['light truck' 'motorcycle' 'medium truck' 'PSV' 'other' 'pickup'\n",
      " 'private car' 'SUV' 'bus' 'bicycle' 'heavy truck']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(metadata_file)\n",
    "# Get distinct values from the 'class' column\n",
    "distinct_classes = df['class'].unique()\n",
    "# Display the result\n",
    "print(distinct_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "373392f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# -------------------------------\n",
    "# Audio Utility Class with MFCC Preprocessing + Normalization\n",
    "# -------------------------------\n",
    "class AudioUtil:\n",
    "    @staticmethod\n",
    "    def open(audio_file):\n",
    "        sig, sr = torchaudio.load(audio_file)\n",
    "        return (sig, sr)\n",
    "\n",
    "    @staticmethod\n",
    "    def rechannel(aud, new_channel):\n",
    "        sig, sr = aud\n",
    "        if sig.shape[0] == new_channel:\n",
    "            return aud\n",
    "        if new_channel == 1:\n",
    "            resig = sig[:1, :]\n",
    "        else:\n",
    "            resig = torch.cat([sig, sig])\n",
    "        return (resig, sr)\n",
    "\n",
    "    @staticmethod\n",
    "    def resample(aud, newsr):\n",
    "        sig, sr = aud\n",
    "        if sr == newsr:\n",
    "            return aud\n",
    "        num_channels = sig.shape[0]\n",
    "        resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1, :])\n",
    "        if num_channels > 1:\n",
    "            retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:, :])\n",
    "            resig = torch.cat([resig, retwo])\n",
    "        return (resig, newsr)\n",
    "\n",
    "    @staticmethod\n",
    "    def pad_trunc(aud, max_ms):\n",
    "        sig, sr = aud\n",
    "        num_rows, sig_len = sig.shape\n",
    "        max_len = sr // 1000 * max_ms\n",
    "        if sig_len > max_len:\n",
    "            sig = sig[:, :max_len]\n",
    "        elif sig_len < max_len:\n",
    "            pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "            pad_end_len = max_len - sig_len - pad_begin_len\n",
    "            pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "            pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "            sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "        return (sig, sr)\n",
    "\n",
    "    @staticmethod\n",
    "    def time_shift(aud, shift_limit):\n",
    "        sig, sr = aud\n",
    "        _, sig_len = sig.shape\n",
    "        shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "        return (sig.roll(shift_amt), sr)\n",
    "\n",
    "    @staticmethod\n",
    "    def mfcc_feature(aud, n_mfcc=40, n_fft=1024, hop_len=512):\n",
    "        sig, sr = aud\n",
    "        mfcc = torchaudio.transforms.MFCC(\n",
    "            sample_rate=sr,\n",
    "            n_mfcc=n_mfcc,\n",
    "            melkwargs={\"n_fft\": n_fft, \"hop_length\": hop_len, \"n_mels\": 64}\n",
    "        )(sig)\n",
    "        mfcc_db = torchaudio.transforms.AmplitudeToDB(top_db=80)(mfcc)\n",
    "        # Normalize\n",
    "        mfcc_db = (mfcc_db - mfcc_db.mean()) / (mfcc_db.std() + 1e-6)\n",
    "        return mfcc_db\n",
    "\n",
    "    @staticmethod\n",
    "    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "        _, n_mels, n_steps = spec.shape\n",
    "        mask_value = spec.mean()\n",
    "        aug_spec = spec\n",
    "        freq_mask_param = max_mask_pct * n_mels\n",
    "        for _ in range(n_freq_masks):\n",
    "            aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "        time_mask_param = max_mask_pct * n_steps\n",
    "        for _ in range(n_time_masks):\n",
    "            aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "        return aug_spec\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f86e8ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Dataset Class Using MFCC + Augmentation\n",
    "# -------------------------------\n",
    "class SoundDS(Dataset):\n",
    "    def __init__(self, df, data_path, train=True):\n",
    "        self.df = df\n",
    "        self.data_path = str(data_path)\n",
    "        self.duration = 4000\n",
    "        self.sr = 22050  # Resample to 22.05 kHz for consistency\n",
    "        self.channel = 1\n",
    "        self.shift_pct = 0.4 if train else 0.0\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        road = self.df.loc[idx, 'road']\n",
    "        slice_file_name = self.df.loc[idx, 'slice_file_name']\n",
    "        relative_path = road + '/' + slice_file_name\n",
    "        audio_file = os.path.join(self.data_path, relative_path)\n",
    "\n",
    "        if not os.path.exists(audio_file):\n",
    "            raise FileNotFoundError(f\"Audio file not found: {audio_file}\")\n",
    "\n",
    "        class_id = self.df.loc[idx, 'classID']\n",
    "\n",
    "        aud = AudioUtil.open(audio_file)\n",
    "        #reaud = AudioUtil.resample(aud, self.sr)\n",
    "        #rechan = AudioUtil.rechannel(reaud, self.channel)\n",
    "        #dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n",
    "        #shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct) if self.train else dur_aud\n",
    "\n",
    "        mfcc_feat = AudioUtil.mfcc_feature(aud, n_mfcc=40, n_fft=1024, hop_len=512)\n",
    "        aug_mfcc = AudioUtil.spectro_augment(mfcc_feat, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2) if self.train else mfcc_feat\n",
    "\n",
    "        return aug_mfcc, class_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f3653eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classID\n",
      "2     0.320366\n",
      "9     0.185355\n",
      "1     0.166590\n",
      "5     0.097025\n",
      "3     0.081922\n",
      "6     0.059497\n",
      "4     0.035240\n",
      "10    0.031121\n",
      "0     0.019222\n",
      "7     0.002746\n",
      "8     0.000915\n",
      "Name: proportion, dtype: float64\n",
      "Global train mean: 0.0008759719785302877, std: 0.9158594608306885\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "import numpy as np\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "\n",
    "print(df['classID'].value_counts(normalize=True))\n",
    "\n",
    "myds = SoundDS(df, download_path)\n",
    "\n",
    "num_items = len(myds)\n",
    "num_train = round(num_items * 0.8)\n",
    "num_val = num_items - num_train\n",
    "train_ds, val_ds = random_split(myds, [num_train, num_val])\n",
    "\n",
    "# Create subset DataFrames with reindexed rows\n",
    "train_df = df.iloc[train_ds.indices].reset_index(drop=True)\n",
    "val_df = df.iloc[val_ds.indices].reset_index(drop=True)\n",
    "\n",
    "train_myds = SoundDS(train_df, download_path, train=True)\n",
    "val_myds = SoundDS(val_df, download_path, train=False)\n",
    "\n",
    "# Weighted sampler for imbalance\n",
    "weights = 1. / df['classID'].value_counts().sort_index().values\n",
    "sample_weights = weights[df['classID'].values[train_ds.indices]]\n",
    "sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_myds, batch_size=16, sampler=sampler)\n",
    "val_dl = torch.utils.data.DataLoader(val_myds, batch_size=16, shuffle=False)\n",
    "\n",
    "# Compute global mean/std using training subset\n",
    "all_sgrams = []\n",
    "for idx in range(len(train_ds)):  # Use range based on train_ds length\n",
    "    sgram, _ = train_myds[idx]\n",
    "    all_sgrams.append(sgram.numpy())\n",
    "train_sgrams = np.concatenate(all_sgrams, axis=0)\n",
    "train_mean = train_sgrams.mean()\n",
    "train_std = train_sgrams.std()\n",
    "print(f'Global train mean: {train_mean}, std: {train_std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61437d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ImprovedAudioClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(128 * 8 * 32, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.lin = nn.Linear(128, 11)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "    \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AudioCNN(nn.Module):\n",
    "    def __init__(self, num_classes=11):\n",
    "        super(AudioCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        # ðŸ”¹ Adaptive pooling ensures consistent shape\n",
    "        self.gap = nn.AdaptiveAvgPool2d((4, 4))  \n",
    "\n",
    "        # After GAP, feature size = 128 * 4 * 4 = 2048\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.gap(x)                  # fixed-size output\n",
    "        x = x.view(x.size(0), -1)        # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "myModel = AudioCNN()\n",
    "next(myModel.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf037f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000 | Loss: 4138.8873 | Train Acc: 16.53% | Val Acc: 3.20%\n",
      "Epoch 2/1000 | Loss: 3853.0577 | Train Acc: 24.60% | Val Acc: 3.66%\n",
      "Epoch 3/1000 | Loss: 3546.6215 | Train Acc: 29.98% | Val Acc: 1.83%\n",
      "Epoch 4/1000 | Loss: 3303.9245 | Train Acc: 35.30% | Val Acc: 15.10%\n",
      "Epoch 5/1000 | Loss: 3111.7862 | Train Acc: 40.50% | Val Acc: 18.31%\n",
      "Epoch 6/1000 | Loss: 3010.0648 | Train Acc: 40.56% | Val Acc: 13.27%\n",
      "Epoch 7/1000 | Loss: 2817.7045 | Train Acc: 44.39% | Val Acc: 25.17%\n",
      "Epoch 8/1000 | Loss: 2818.5735 | Train Acc: 43.31% | Val Acc: 20.14%\n",
      "Epoch 9/1000 | Loss: 2755.5005 | Train Acc: 44.45% | Val Acc: 18.99%\n",
      "Epoch 10/1000 | Loss: 2669.4436 | Train Acc: 44.68% | Val Acc: 23.57%\n",
      "Epoch 11/1000 | Loss: 2667.3370 | Train Acc: 45.25% | Val Acc: 20.37%\n",
      "Epoch 12/1000 | Loss: 2604.2112 | Train Acc: 47.48% | Val Acc: 11.21%\n",
      "Epoch 13/1000 | Loss: 2640.2897 | Train Acc: 45.31% | Val Acc: 16.25%\n",
      "Epoch 14/1000 | Loss: 2571.6919 | Train Acc: 45.37% | Val Acc: 15.56%\n",
      "Epoch 15/1000 | Loss: 2521.9136 | Train Acc: 48.74% | Val Acc: 31.35%\n",
      "Epoch 16/1000 | Loss: 2447.0689 | Train Acc: 50.11% | Val Acc: 10.30%\n",
      "Epoch 17/1000 | Loss: 2467.8960 | Train Acc: 49.31% | Val Acc: 22.20%\n",
      "Epoch 18/1000 | Loss: 2369.2923 | Train Acc: 50.06% | Val Acc: 19.91%\n",
      "Epoch 19/1000 | Loss: 2444.5209 | Train Acc: 49.66% | Val Acc: 12.59%\n",
      "Epoch 20/1000 | Loss: 2366.9286 | Train Acc: 50.46% | Val Acc: 32.27%\n",
      "Epoch 21/1000 | Loss: 2315.6162 | Train Acc: 52.92% | Val Acc: 18.31%\n",
      "Epoch 22/1000 | Loss: 2332.8073 | Train Acc: 51.09% | Val Acc: 21.05%\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.01 MiB for an array with shape (264600, 1) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mFinished Training\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     52\u001b[39m num_epochs=\u001b[32m1000\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmyModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mtraining\u001b[39m\u001b[34m(model, train_dl, val_dl, num_epochs, device)\u001b[39m\n\u001b[32m      7\u001b[39m model.train()\n\u001b[32m      8\u001b[39m running_loss, correct, total = \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\SonusAI\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\SonusAI\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\SonusAI\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mSoundDS.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAudio file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maudio_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m class_id = \u001b[38;5;28mself\u001b[39m.df.loc[idx, \u001b[33m'\u001b[39m\u001b[33mclassID\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m aud = \u001b[43mAudioUtil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m#reaud = AudioUtil.resample(aud, self.sr)\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m#rechan = AudioUtil.rechannel(reaud, self.channel)\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m#dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m#shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct) if self.train else dur_aud\u001b[39;00m\n\u001b[32m     34\u001b[39m mfcc_feat = AudioUtil.mfcc_feature(aud, n_mfcc=\u001b[32m40\u001b[39m, n_fft=\u001b[32m1024\u001b[39m, hop_len=\u001b[32m512\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mAudioUtil.open\u001b[39m\u001b[34m(audio_file)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mopen\u001b[39m(audio_file):\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     sig, sr = \u001b[43mtorchaudio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (sig, sr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\SonusAI\\venv\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:205\u001b[39m, in \u001b[36mget_load_func.<locals>.load\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[32m    129\u001b[39m \n\u001b[32m    130\u001b[39m \u001b[33;03mBy default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    202\u001b[39m \u001b[33;03m        `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    204\u001b[39m backend = dispatcher(uri, \u001b[38;5;28mformat\u001b[39m, backend)\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\SonusAI\\venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile.py:27\u001b[39m, in \u001b[36mSoundfileBackend.load\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m     19\u001b[39m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     buffer_size: \u001b[38;5;28mint\u001b[39m = \u001b[32m4096\u001b[39m,\n\u001b[32m     26\u001b[39m ) -> Tuple[torch.Tensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msoundfile_backend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\SonusAI\\venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:230\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001b[39m\n\u001b[32m    227\u001b[39m         dtype = _SUBTYPE2DTYPE[file_.subtype]\n\u001b[32m    229\u001b[39m     frames = file_._prepare_read(frame_offset, \u001b[38;5;28;01mNone\u001b[39;00m, num_frames)\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     waveform = \u001b[43mfile_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malways_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m     sample_rate = file_.samplerate\n\u001b[32m    233\u001b[39m waveform = torch.from_numpy(waveform)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\SonusAI\\venv\\Lib\\site-packages\\soundfile.py:938\u001b[39m, in \u001b[36mSoundFile.read\u001b[39m\u001b[34m(self, frames, dtype, always_2d, fill_value, out)\u001b[39m\n\u001b[32m    936\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    937\u001b[39m     frames = \u001b[38;5;28mself\u001b[39m._check_frames(frames, fill_value)\n\u001b[32m--> \u001b[39m\u001b[32m938\u001b[39m     out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_empty_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malways_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    940\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m frames < \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m frames > \u001b[38;5;28mlen\u001b[39m(out):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\SonusAI\\venv\\Lib\\site-packages\\soundfile.py:1372\u001b[39m, in \u001b[36mSoundFile._create_empty_array\u001b[39m\u001b[34m(self, frames, always_2d, dtype)\u001b[39m\n\u001b[32m   1370\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1371\u001b[39m     shape = frames,\n\u001b[32m-> \u001b[39m\u001b[32m1372\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mC\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 1.01 MiB for an array with shape (264600, 1) and data type float32"
     ]
    }
   ],
   "source": [
    "def training(model, train_dl, val_dl, num_epochs, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=5)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0, 0, 0\n",
    "        for inputs, labels in train_dl:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "\n",
    "        train_acc = 100. * correct / total\n",
    "        val_acc = evaluate_model(model, val_dl, device)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "              f\"Loss: {running_loss:.4f} | \"\n",
    "              f\"Train Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "\n",
    "    \n",
    "def evaluate_model(model, data_loader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        return 100. * correct / total\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "num_epochs=1000\n",
    "training(myModel, train_dl, val_dl, num_epochs, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa25b230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model outside the training function\n",
    "torch.save(myModel.state_dict(), 'audio_classifier_model2.pth')\n",
    "print(\"Model saved as 'audio_classifier_model.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab4e4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, data_loader, train_mean, train_std):\n",
    "    \"\"\"\n",
    "    CPU-optimized inference function\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            # Normalize inputs\n",
    "            inputs = (inputs - train_mean) / train_std\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            _, prediction = torch.max(outputs, 1)\n",
    "            correct_prediction += (prediction == labels).sum().item()\n",
    "            total_prediction += prediction.shape[0]\n",
    "    \n",
    "    return correct_prediction / total_prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
