{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec876b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Prepare training data from Metadata file\n",
    "# ----------------------------\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "download_path = Path('data\\\\DATA\\\\output_chunks')\n",
    "\n",
    "# Read metadata file\n",
    "metadata_file = download_path / 'metadata.csv'\n",
    "df = pd.read_csv(metadata_file)\n",
    "df.head()\n",
    "\n",
    "# Construct file path by concatenating road and slice_file_name\n",
    "df['relative_path'] = '/road/' + df['road'].astype(str) + '/' + df['slice_file_name'].astype(str)\n",
    "\n",
    "# Take relevant columns\n",
    "df = df[['relative_path', 'classID']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976e9081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(metadata_file)\n",
    "\n",
    "# Get distinct values from the 'class' column\n",
    "distinct_classes = df['class'].unique()\n",
    "\n",
    "\n",
    "# Display the result\n",
    "print(distinct_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373392f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from IPython.display import Audio\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "class AudioUtil:\n",
    "    @staticmethod\n",
    "    def open(audio_file):\n",
    "        sig, sr = torchaudio.load(audio_file)\n",
    "        return (sig, sr)\n",
    "\n",
    "    @staticmethod\n",
    "    def rechannel(aud, new_channel):\n",
    "        sig, sr = aud\n",
    "        if (sig.shape[0] == new_channel):\n",
    "            return aud\n",
    "        if (new_channel == 1):\n",
    "            resig = sig[:1, :]\n",
    "        else:\n",
    "            resig = torch.cat([sig, sig])\n",
    "        return ((resig, sr))\n",
    "\n",
    "    @staticmethod\n",
    "    def resample(aud, newsr):\n",
    "        sig, sr = aud\n",
    "        if (sr == newsr):\n",
    "            return aud\n",
    "        num_channels = sig.shape[0]\n",
    "        resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n",
    "        if (num_channels > 1):\n",
    "            retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n",
    "            resig = torch.cat([resig, retwo])\n",
    "        return ((resig, newsr))\n",
    "\n",
    "    @staticmethod\n",
    "    def pad_trunc(aud, max_ms):\n",
    "        sig, sr = aud\n",
    "        num_rows, sig_len = sig.shape\n",
    "        max_len = sr//1000 * max_ms\n",
    "        if (sig_len > max_len):\n",
    "            sig = sig[:,:max_len]\n",
    "        elif (sig_len < max_len):\n",
    "            pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "            pad_end_len = max_len - sig_len - pad_begin_len\n",
    "            pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "            pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "            sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "        return (sig, sr)\n",
    "\n",
    "    @staticmethod\n",
    "    def time_shift(aud, shift_limit):\n",
    "        sig,sr = aud\n",
    "        _, sig_len = sig.shape\n",
    "        shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "        return (sig.roll(shift_amt), sr)\n",
    "\n",
    "    @staticmethod\n",
    "    def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
    "        sig,sr = aud\n",
    "        top_db = 80\n",
    "        spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "        return (spec)\n",
    "\n",
    "    @staticmethod\n",
    "    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "        _, n_mels, n_steps = spec.shape\n",
    "        mask_value = spec.mean()\n",
    "        aug_spec = spec\n",
    "        freq_mask_param = max_mask_pct * n_mels\n",
    "        for _ in range(n_freq_masks):\n",
    "            aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "        time_mask_param = max_mask_pct * n_steps\n",
    "        for _ in range(n_time_masks):\n",
    "            aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "        return aug_spec\n",
    "\n",
    "    @staticmethod\n",
    "    def image_augment(spec):\n",
    "        # Convert tensor to numpy for albumentations\n",
    "        spec_np = spec.numpy().transpose(1, 2, 0)  # Assuming spec is (C, H, W), adjust if needed\n",
    "        transform = A.Compose([\n",
    "            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=30, p=0.5),\n",
    "            A.GridDistortion(p=0.5),\n",
    "            A.Cutout(num_holes=8, max_h_size=8, max_w_size=8, p=0.5),\n",
    "        ])\n",
    "        augmented = transform(image=spec_np)\n",
    "        aug_spec = augmented['image'].transpose(2, 0, 1)  # Back to (C, H, W)\n",
    "        return torch.from_numpy(aug_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86e8ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "\n",
    "class SoundDS(Dataset):\n",
    "    def __init__(self, df, data_path):\n",
    "        self.df = df\n",
    "        self.data_path = str(data_path)\n",
    "        self.duration = 4000\n",
    "        self.sr = 44100\n",
    "        self.channel = 2\n",
    "        self.shift_pct = 0.4\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Construct the full path using 'road' and 'slice_file_name'\n",
    "        road = self.df.loc[idx, 'road']\n",
    "        slice_file_name = self.df.loc[idx, 'slice_file_name']\n",
    "        relative_path = road + '/' + slice_file_name\n",
    "        audio_file = os.path.join(self.data_path, relative_path)\n",
    "        \n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(audio_file):\n",
    "            raise FileNotFoundError(f\"Audio file not found: {audio_file}\")\n",
    "        \n",
    "        # Get the Class ID\n",
    "        class_id = self.df.loc[idx, 'classID']\n",
    "\n",
    "        aud = AudioUtil.open(audio_file)\n",
    "        reaud = AudioUtil.resample(aud, self.sr)\n",
    "        rechan = AudioUtil.rechannel(reaud, self.channel)\n",
    "        dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n",
    "        shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n",
    "        sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024)\n",
    "        aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
    "        aug_sgram = AudioUtil.image_augment(aug_sgram)\n",
    "        return aug_sgram, class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d5c44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "# Neural Network Model (Dual Backbone Architecture)\n",
    "class UrbanSoundModel(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained_global=True):\n",
    "        super(UrbanSoundModel, self).__init__()\n",
    "        \n",
    "        # Global Feature Extractor (TALNet-like, pretrained on AudioSet)\n",
    "        self.global_conv_blocks = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=(3, 3), padding=1),\n",
    "            nn.GroupNorm(8, 64),  # Group Normalization\n",
    "            nn.Mish(),  # Mish activation\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), padding=1),\n",
    "            nn.GroupNorm(16, 128),\n",
    "            nn.Mish(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(128, 256, kernel_size=(3, 3), padding=1),\n",
    "            nn.GroupNorm(32, 256),\n",
    "            nn.Mish(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(256, 512, kernel_size=(3, 3), padding=1),\n",
    "            nn.GroupNorm(64, 512),\n",
    "            nn.Mish(),\n",
    "            nn.MaxPool2d((2, 2))\n",
    "        )\n",
    "        self.global_rnn = nn.GRU(512, 256, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # Specific Feature Extractor (Modified TALNet)\n",
    "        self.specific_conv_blocks = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=(3, 3), padding=1),\n",
    "            nn.GroupNorm(8, 64),  # Group Normalization\n",
    "            nn.Mish(),  # Mish activation\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), padding=1),\n",
    "            nn.GroupNorm(16, 128),\n",
    "            nn.Mish(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(128, 256, kernel_size=(3, 3), padding=1),\n",
    "            nn.GroupNorm(32, 256),\n",
    "            nn.Mish(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(256, 512, kernel_size=(3, 3), padding=1),\n",
    "            nn.GroupNorm(64, 512),\n",
    "            nn.Mish(),\n",
    "            nn.MaxPool2d((2, 2))\n",
    "        )\n",
    "        self.specific_attention = nn.MultiheadAttention(512, num_heads=8, batch_first=True)\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1024, 512),  # 512 (GRU) + 512 (Attention)\n",
    "            nn.Mish(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes),\n",
    "            nn.Sigmoid()  # Output probabilities\n",
    "        )\n",
    "        \n",
    "        # Gradient Centralization for convolutional layers\n",
    "        self.apply(self._init_gc)\n",
    "\n",
    "    def _init_gc(self, m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.utils.weight_norm(m, name='weight')\n",
    "            m.weight_g = nn.Parameter(m.weight_g - m.weight_g.mean())\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Global Feature Extraction\n",
    "        global_features = self.global_conv_blocks(x)\n",
    "        global_features = global_features.transpose(1, 2)  # (batch, time, freq, channels) -> (batch, time, channels)\n",
    "        global_features = global_features.contiguous().view(global_features.size(0), global_features.size(1), -1)\n",
    "        global_features, _ = self.global_rnn(global_features)\n",
    "        global_features = global_features[:, -1, :]  # Take the last time step\n",
    "\n",
    "        # Specific Feature Extraction\n",
    "        specific_features = self.specific_conv_blocks(x)\n",
    "        specific_features = specific_features.transpose(1, 2)\n",
    "        specific_features = specific_features.contiguous().view(specific_features.size(0), specific_features.size(1), -1)\n",
    "        specific_features, _ = self.specific_attention(specific_features, specific_features, specific_features)\n",
    "        specific_features = specific_features[:, -1, :]  # Take the last time step\n",
    "\n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat((global_features, specific_features), dim=1)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        output = self.fc(combined_features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b34414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def training(model, train_dl, num_epochs, device):\n",
    "    criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for multi-label\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n",
    "                                             steps_per_epoch=int(len(train_dl)),\n",
    "                                             epochs=num_epochs,\n",
    "                                             anneal_strategy='linear')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_prediction = 0\n",
    "        total_prediction = 0\n",
    "\n",
    "        for i, data in enumerate(train_dl):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            correct_prediction += (predicted == labels).sum().item()\n",
    "            total_prediction += labels.numel()\n",
    "\n",
    "        avg_loss = running_loss / len(train_dl)\n",
    "        acc = correct_prediction / total_prediction\n",
    "        print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f783fb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference/Evaluation Function\n",
    "def inference(model, val_dl, device):\n",
    "    model.eval()\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "    \n",
    "    precision = Precision(task=\"multilabel\", num_labels=model.fc[-2].out_features).to(device)\n",
    "    recall = Recall(task=\"multilabel\", num_labels=model.fc[-2].out_features).to(device)\n",
    "    f1 = F1Score(task=\"multilabel\", num_labels=model.fc[-2].out_features).to(device)\n",
    "    confmat = ConfusionMatrix(task=\"multilabel\", num_labels=model.fc[-2].out_features).to(device)\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in val_dl:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device).float()\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "\n",
    "            correct_prediction += (predicted == labels).sum().item()\n",
    "            total_prediction += labels.numel()\n",
    "            \n",
    "            precision.update(predicted, labels.int())\n",
    "            recall.update(predicted, labels.int())\n",
    "            f1.update(predicted, labels.int())\n",
    "            confmat.update(predicted, labels.int())\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(outputs.cpu().numpy())\n",
    "\n",
    "    acc = correct_prediction / total_prediction\n",
    "    prec = precision.compute().item()\n",
    "    rec = recall.compute().item()\n",
    "    f1_score = f1.compute().item()\n",
    "    conf_matrix = confmat.compute().cpu().numpy()\n",
    "\n",
    "    print(f'Accuracy: {acc:.4f}, Total items: {total_prediction}')\n",
    "    print(f'Precision: {prec:.4f}, Recall: {rec:.4f}, F1-Score: {f1_score:.4f}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'f1_score': f1_score,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'predictions': np.array(all_predictions),\n",
    "        'labels': np.array(all_labels),\n",
    "        'probabilities': np.array(all_probs)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c65663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data (example placeholders)\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    download_path = Path('data\\\\DATA\\\\output_chunks')\n",
    "    metadata_file = download_path / 'metadata.csv'\n",
    "    df = pd.read_csv(metadata_file)\n",
    "    df['relative_path'] = '/road/' + df['road'].astype(str) + '/' + df['slice_file_name'].astype(str)\n",
    "    df = df[['relative_path', 'classID']]\n",
    "\n",
    "    # Dataset and DataLoader\n",
    "    dataset = SoundDS(df, download_path)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    train_dl = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_dl = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Model\n",
    "    model = UrbanSoundModel(num_classes=11).to(device)  # Adjust num_classes based on dataset\n",
    "\n",
    "    # Training\n",
    "    num_epochs = 50\n",
    "    training(model, train_dl, num_epochs, device)\n",
    "\n",
    "    # Evaluation\n",
    "    results = inference(model, val_dl, device)\n",
    "\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), 'urban_sound_model.pth')\n",
    "    print(\"Model saved as 'urban_sound_model.pth'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
