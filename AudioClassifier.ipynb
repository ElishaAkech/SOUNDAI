{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ceadd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\SonusAI\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import ReLU\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.multiprocessing\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import models\n",
    "from torch import optim\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from colorama import Fore\n",
    "from IPython.display import Audio, display\n",
    "from torchsummary import summary\n",
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer as timer\n",
    "import psutil\n",
    "\n",
    "writer_path = 'runs/logger_classifier_beta_v2'\n",
    "# writer to log to tensorboard\n",
    "writer = SummaryWriter(writer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "172810d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Display spectrogram of an audio waveform.\n",
    "@param waveform: Numpy waveform of sound\n",
    "@param sample_rate: Sound sample rate\n",
    "\"\"\"\n",
    "def audio_display_spectrogram(waveform, sample_rate, title=\"Spectrogram\", xlim=None) -> None:\n",
    "    waveform = waveform.numpy()\n",
    "    num_channels, _ = waveform.shape\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    if num_channels == 1:\n",
    "        axes = [axes]\n",
    "    for c in range(num_channels):\n",
    "        axes[c].specgram(waveform[c], Fs=sample_rate)\n",
    "        if num_channels > 1:\n",
    "            axes[c].set_ylabel(f\"Channel {c+1}\")\n",
    "        if xlim:\n",
    "            axes[c].set_xlim(xlim)\n",
    "    figure.suptitle(title)\n",
    "    plt.show(block=False)\n",
    "\n",
    "\"\"\"\n",
    "Play sound of audio waveform.\n",
    "@param waveform: Numpy waveform of sound\n",
    "@param sample_rate: Sound sample rate\n",
    "\"\"\"\n",
    "def audio_play(waveform, sample_rate) -> None:\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, _ = waveform.shape\n",
    "    if num_channels == 1:\n",
    "        display(Audio(waveform[0], rate=sample_rate))\n",
    "    elif num_channels == 2:\n",
    "        display(Audio((waveform[0], waveform[1]), rate=sample_rate))\n",
    "    else:\n",
    "        raise ValueError(\"Waveform with more than 2 channels are not supported.\")\n",
    "\n",
    "\"\"\"\n",
    "Display a spectrogram image\n",
    "@param img: Spectrogram of sound\n",
    "@param one_channel: Whenever image is grey or has color (RGB) \n",
    "\"\"\"\n",
    "def image_display_spectrogram(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\"\"\"\n",
    "Display all the spectrogram of sounds within a batch\n",
    "@param batches: Batch of data from a dataloader \n",
    "\"\"\"\n",
    "def batches_display(batches, writer_path):\n",
    "    dataiter = iter(batches)\n",
    "    images, _ = next(dataiter)\n",
    "    # create grid of images\n",
    "    img_grid = torchvision.utils.make_grid(images)\n",
    "    # show images\n",
    "    image_display_spectrogram(img_grid, one_channel=False)\n",
    "    # write to tensorboard\n",
    "    writer.add_image(writer_path, img_grid)\n",
    "\n",
    "\"\"\"\n",
    "Log the size of each batch\n",
    "@param batches: Batch of data from a dataloader \n",
    "\"\"\"\n",
    "def batches_log_shape(batches):\n",
    "    i = 0\n",
    "    for curr_batch_image, _ in batches:\n",
    "        print(Fore.GREEN, '[', '='*(i+1), ' '*(len(batches)-i-1), f'] Generated batch {i} with {len(curr_batch_image)} images')\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f579e8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available\n",
      "No Metal apple device\n",
      "\u001b[95mUsing cpu device for computation.\n",
      "\u001b[32mComputation workers count set to 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\SonusAI\\venv\\Lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (389) may be set too high. Or, the value for `n_freqs` (513) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torchaudio.transforms' has no attribute 'Resize'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 123\u001b[39m\n\u001b[32m    107\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m waveform, target\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# --- Audio Transformations ---\u001b[39;00m\n\u001b[32m    110\u001b[39m \n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# Define the sequence of transformations to be applied to the audio waveforms\u001b[39;00m\n\u001b[32m    112\u001b[39m audio_transform = nn.Sequential(\n\u001b[32m    113\u001b[39m     \u001b[38;5;66;03m# 1. Resample to the target sample rate\u001b[39;00m\n\u001b[32m    114\u001b[39m     torchaudio.transforms.Resample(orig_freq=\u001b[32m44100\u001b[39m, new_freq=TARGET_SAMPLE_RATE), \u001b[38;5;66;03m# Note: Set orig_freq to your dataset's actual sample rate\u001b[39;00m\n\u001b[32m    115\u001b[39m \n\u001b[32m    116\u001b[39m     \u001b[38;5;66;03m# 2. Convert to a Mel Spectrogram\u001b[39;00m\n\u001b[32m    117\u001b[39m     torchaudio.transforms.MelSpectrogram(sample_rate=TARGET_SAMPLE_RATE, n_fft=\u001b[32m1024\u001b[39m, hop_length=\u001b[32m512\u001b[39m, n_mels=IMAGE_SIZE[\u001b[32m0\u001b[39m]),\n\u001b[32m    118\u001b[39m \n\u001b[32m    119\u001b[39m     \u001b[38;5;66;03m# 3. Convert to a logarithmic scale (decibels)\u001b[39;00m\n\u001b[32m    120\u001b[39m     torchaudio.transforms.AmplitudeToDB(),\n\u001b[32m    121\u001b[39m \n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# 4. Resize the spectrogram to the desired final image size\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m     \u001b[43mtorchaudio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mResize\u001b[49m(IMAGE_SIZE),\n\u001b[32m    124\u001b[39m \n\u001b[32m    125\u001b[39m     \u001b[38;5;66;03m# 5. Convert single-channel spectrogram to 3 channels by duplicating the channel\u001b[39;00m\n\u001b[32m    126\u001b[39m     nn.Lambda(\u001b[38;5;28;01mlambda\u001b[39;00m x: x.repeat(CHANNEL_COUNT, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m    127\u001b[39m )\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# Move transforms to the selected compute device for potential speed-up\u001b[39;00m\n\u001b[32m    130\u001b[39m audio_transform = audio_transform.to(compute_unit)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'torchaudio.transforms' has no attribute 'Resize'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from pathlib import Path\n",
    "from colorama import Fore, Style\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- Constants ---\n",
    "NUM_WORKERS = 4\n",
    "QUICK_DEV = False\n",
    "DATASET_PATH = 'data'\n",
    "\n",
    "# Load smaller database for faster loading time\n",
    "if QUICK_DEV:\n",
    "    DATASET_PATH = 'data' # You might want a different path for a smaller dev dataset\n",
    "\n",
    "# Constants for audio processing and model input\n",
    "IMAGE_SIZE = (389, 515)  # The target size for the spectrogram (height, width)\n",
    "CHANNEL_COUNT = 3        # Target channels for the model\n",
    "TARGET_SAMPLE_RATE = 22050 # Standardize all audio to this sample rate\n",
    "\n",
    "ATTRIBUTION = [\"bus\", \"car\", \"motorcycle\", \"pedestrian\", \"truck\"]\n",
    "SAVING_PATH = \"../models/model_binary_beta_v1\"\n",
    "ACCURACY_THRESHOLD = 85\n",
    "\n",
    "AUDIO_EXTENSIONS = {'.wav', '.mp3', '.flac'}\n",
    "\n",
    "# --- Device Setup ---\n",
    "print(\"CUDA available\" if torch.cuda.is_available() else \"CUDA not available\")\n",
    "print(\"Metal apple device detected\" if torch.backends.mps.is_built() else \"No Metal apple device\")\n",
    "\n",
    "# Get CPU or GPU device for training.\n",
    "if torch.cuda.is_available():\n",
    "    compute_unit = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    compute_unit = torch.device(\"mps\")\n",
    "else:\n",
    "    compute_unit = torch.device(\"cpu\")\n",
    "\n",
    "# see https://github.com/pytorch/pytorch/issues/11201\n",
    "if os.name != 'nt': # file_system sharing strategy is not available on Windows\n",
    "    torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "print(Fore.LIGHTMAGENTA_EX + f\"Using {compute_unit} device for computation.\")\n",
    "print(Fore.GREEN + f\"Computation workers count set to {NUM_WORKERS}\")\n",
    "\n",
    "\n",
    "# --- Custom Audio Dataset ---\n",
    "class AudioFolder(Dataset):\n",
    "    \"\"\"\n",
    "    A custom Dataset class for audio files, structured like ImageFolder.\n",
    "    Assumes that root directory contains class-named subdirectories with audio files.\n",
    "    \"\"\"\n",
    "    def __init__(self, root, transform=None, sample_rate=22050, extensions=None):\n",
    "        self.root = Path(root)\n",
    "        self.transform = transform\n",
    "        self.sample_rate = sample_rate\n",
    "        self.extensions = extensions if extensions else AUDIO_EXTENSIONS\n",
    "\n",
    "        self.classes, self.class_to_idx = self._find_classes(self.root)\n",
    "        self.samples = self._make_dataset(self.root, self.class_to_idx, self.extensions)\n",
    "\n",
    "        if not self.samples:\n",
    "            raise RuntimeError(f\"Found 0 files in subfolders of: {self.root}. \"\n",
    "                               f\"Supported extensions are: {', '.join(self.extensions)}\")\n",
    "\n",
    "    def _find_classes(self, dir):\n",
    "        classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n",
    "        classes.sort()\n",
    "        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "        return classes, class_to_idx\n",
    "\n",
    "    def _make_dataset(self, dir, class_to_idx, extensions):\n",
    "        instances = []\n",
    "        dir = os.path.expanduser(dir)\n",
    "        for target_class in sorted(class_to_idx.keys()):\n",
    "            class_index = class_to_idx[target_class]\n",
    "            target_dir = os.path.join(dir, target_class)\n",
    "            if not os.path.isdir(target_dir):\n",
    "                continue\n",
    "            for root, _, fnames in sorted(os.walk(target_dir, followlinks=True)):\n",
    "                for fname in sorted(fnames):\n",
    "                    path = os.path.join(root, fname)\n",
    "                    if path.lower().endswith(tuple(extensions)):\n",
    "                        item = (path, class_index)\n",
    "                        instances.append(item)\n",
    "        return instances\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        # Load audio file\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file {path}: {e}\")\n",
    "            # Return a dummy tensor and target if a file is corrupt\n",
    "            return torch.zeros((CHANNEL_COUNT, *IMAGE_SIZE)), target\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "\n",
    "        return waveform, target\n",
    "\n",
    "# --- Audio Transformations ---\n",
    "\n",
    "# Define the sequence of transformations to be applied to the audio waveforms\n",
    "audio_transform = nn.Sequential(\n",
    "    # 1. Resample to the target sample rate\n",
    "    torchaudio.transforms.Resample(orig_freq=44100, new_freq=TARGET_SAMPLE_RATE), # Note: Set orig_freq to your dataset's actual sample rate\n",
    "\n",
    "    # 2. Convert to a Mel Spectrogram\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=TARGET_SAMPLE_RATE, n_fft=1024, hop_length=512, n_mels=IMAGE_SIZE[0]),\n",
    "\n",
    "    # 3. Convert to a logarithmic scale (decibels)\n",
    "    torchaudio.transforms.AmplitudeToDB(),\n",
    "\n",
    "    # 4. Resize the spectrogram to the desired final image size\n",
    "    torchaudio.transforms.Resize(IMAGE_SIZE),\n",
    "\n",
    "    # 5. Convert single-channel spectrogram to 3 channels by duplicating the channel\n",
    "    nn.Lambda(lambda x: x.repeat(CHANNEL_COUNT, 1, 1))\n",
    ")\n",
    "\n",
    "# Move transforms to the selected compute device for potential speed-up\n",
    "audio_transform = audio_transform.to(compute_unit)\n",
    "\n",
    "\n",
    "# --- Load the Dataset ---\n",
    "print(Fore.LIGHTMAGENTA_EX + f\"Loading audio files from dataset at {DATASET_PATH}\")\n",
    "\n",
    "try:\n",
    "    # Instantiate the custom dataset\n",
    "    audio_dataset = AudioFolder(\n",
    "        root=DATASET_PATH,\n",
    "        transform=audio_transform,\n",
    "        sample_rate=TARGET_SAMPLE_RATE\n",
    "    )\n",
    "    print(Fore.GREEN + f\"Successfully loaded dataset with {len(audio_dataset)} audio files from {len(audio_dataset.classes)} classes.\")\n",
    "    print(f\"Classes found: {audio_dataset.classes}\")\n",
    "except (RuntimeError, FileNotFoundError) as e:\n",
    "    print(Fore.RED + f\"Error loading dataset: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238d6723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data transformation\n",
    "transform=transforms.ToTensor() \n",
    "\n",
    "# Load the dataset\n",
    "print(Fore.LIGHTMAGENTA_EX + f\"Loading images from dataset at {DATASET_PATH}\")\n",
    "dataset = datasets.ImageFolder(DATASET_PATH, transform=transform)\n",
    "\n",
    "# train / test split\n",
    "val_ratio = 0.2\n",
    "val_size = int(val_ratio * len(dataset))\n",
    "train_size = len(dataset) - val_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "print(Fore.GREEN + f\"{train_size} images for training, {val_size} images for validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaaf8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "\n",
    "# Load into batches\n",
    "train_batches = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=NUM_WORKERS,\n",
    "                                           pin_memory=False) # switch to True if using collate\n",
    "\n",
    "val_batches = torch.utils.data.DataLoader(val_dataset,\n",
    "                                         batch_size=batch_size*2,\n",
    "                                         num_workers=NUM_WORKERS,\n",
    "                                         pin_memory=False) # switch to True if using collate\n",
    "\n",
    "print(Fore.LIGHTMAGENTA_EX + f\"Dataset loaded in batches.\")\n",
    "print(Fore.GREEN + f\"Batch set to {batch_size} for training\")\n",
    "print(Fore.GREEN + f\"Batch set to {batch_size*2} for validation\")\n",
    "batches_display(val_batches, writer_path=writer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9197901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN as sequential\n",
    "model_binary_v1_arch = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "    nn.Conv2d(10, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=480,\n",
    "              out_features=2),\n",
    ")\n",
    "\n",
    "# define CNN as sequential\n",
    "class neuralNetworkV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1) \n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1) \n",
    "        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(10, 10, kernel_size=3, stride=2, padding=1)\n",
    "        self.pooling = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(in_features=480, out_features=2)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pooling(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pooling(x)\n",
    "        x = self.relu(self.conv4(x))\n",
    "        x = self.flatten(x)\n",
    "        try:\n",
    "            x = self.linear(x)\n",
    "        except Exception as e:\n",
    "            print(Fore.RED + f\"Error : Linear block should take support shape of {x.shape} for in_features.\")\n",
    "        return x\n",
    "\n",
    "#selected_model = model_binary_v1_arch.to(compute_unit)\n",
    "selected_model = neuralNetworkV1()\n",
    "\n",
    "# Add CNN info to tensorboard\n",
    "train_images_sample, _ = next(iter(train_batches))\n",
    "writer.add_graph(selected_model, train_images_sample)\n",
    "\n",
    "print(Fore.LIGHTMAGENTA_EX + f\"Training dataloader shape :\")\n",
    "print(Fore.GREEN + f\"({len(train_batches)}, {len(train_images_sample)}, {len(train_images_sample[0])}, {len(train_images_sample[0][0])}, {len(train_images_sample[0][0][0])})\")\n",
    "\n",
    "print(Fore.LIGHTMAGENTA_EX + \"Model summary : \" + Fore.GREEN)\n",
    "print(summary(selected_model, (CHANNEL_COUNT, IMAGE_SIZE[0], IMAGE_SIZE[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7451e991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display total time training\n",
    "def display_training_time(start, end, device):\n",
    "    total_time = end - start\n",
    "    print(Fore.LIGHTMAGENTA_EX + f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time\n",
    "\n",
    "# Calculate accuracy\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    \"\"\"Calculates accuracy between truth labels and predictions.\n",
    "    Args:\n",
    "        y_true (torch.Tensor): Truth labels for predictions.\n",
    "        y_pred (torch.Tensor): Predictions to be compared to predictions.\n",
    "    Returns:\n",
    "        [torch.float]: Accuracy value between y_true and y_pred, e.g. 78.45\n",
    "    \"\"\"\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "# Display training infos for each epochs\n",
    "def display_training_infos(epoch, val_loss, train_loss, accuracy):\n",
    "    val_loss = round(val_loss.item(), 3)\n",
    "    train_loss = round(train_loss.item(), 3)\n",
    "    accuracy = round(accuracy, 2)\n",
    "    print(Fore.GREEN + f\"Epoch : {epoch}, Training loss : {train_loss}, Validation loss : {val_loss}, Accuracy : {accuracy} %\")\n",
    "\n",
    "# Check memory usage excess\n",
    "def check_memory():\n",
    "    mem_percent = psutil.virtual_memory().percent\n",
    "    swap_percent = psutil.swap_memory().percent\n",
    "    if mem_percent >= 90:\n",
    "        print(Fore.YELLOW + f\"WARNING : Reached {mem_percent} memory usage !\")\n",
    "        os.system(f'say \"Memory usage high\"')\n",
    "    if swap_percent >= 90:\n",
    "        print(Fore.YELLOW + f\"WARNING : Reached {mem_percent} memory usage !\")\n",
    "        os.system(f'say \"Swap usage high\"')\n",
    "    if mem_percent >= 95 and swap_percent >= 95:\n",
    "        print(Fore.RED + f\"ABORTING : Memory and Swap full !\")\n",
    "        os.system(f'say \"Aborting training\"')\n",
    "        raise MemoryError\n",
    "\n",
    "def train_neural_net(epochs, model, loss_func, optimizer, train_batches, val_batches):\n",
    "    last_loss = 0\n",
    "    final_accuracy = 0\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        # check memory and swap usage\n",
    "        check_memory()\n",
    "        # training mode\n",
    "        model.train()\n",
    "        with torch.enable_grad():\n",
    "            train_loss = 0\n",
    "            for images, labels in train_batches:\n",
    "                predictions = model(images)\n",
    "                loss = loss_func(predictions, labels)\n",
    "                train_loss += loss\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            train_loss /= len(train_batches)\n",
    "            writer.add_scalar(\"training loss\", train_loss, epoch)\n",
    "        # evaluation mode\n",
    "        val_loss, val_accuracy = 0, 0\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            for images, labels in val_batches:\n",
    "                #images, labels = images.to(compute_unit), labels.to(compute_unit)\n",
    "                predictions = model(images)\n",
    "                val_loss += loss_func(predictions, labels)\n",
    "                val_accuracy += accuracy_fn(y_true=labels, y_pred=predictions.argmax(dim=1))\n",
    "            val_loss /= len(val_batches)\n",
    "            val_accuracy /= len(val_batches)\n",
    "            writer.add_scalar(\"validation loss\", val_loss, epoch)\n",
    "            final_accuracy = val_accuracy\n",
    "        display_training_infos(epoch+1, val_loss, train_loss, val_accuracy)\n",
    "        writer.add_scalar(\"accuracy\", val_accuracy, epoch)\n",
    "        if val_accuracy >= ACCURACY_THRESHOLD:\n",
    "            break\n",
    "        last_loss = val_loss\n",
    "    return final_accuracy\n",
    "\n",
    "MAX_EPOCHS = 300\n",
    "LEARNING_RATE = 0.01\n",
    "GRADIENT_MOMENTUM = 0.90\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(selected_model.parameters(), lr=LEARNING_RATE, momentum=GRADIENT_MOMENTUM)\n",
    "\n",
    "print(Fore.LIGHTMAGENTA_EX + \"Model ready : \")\n",
    "print(Fore.GREEN, f\"Learning rate set to : {LEARNING_RATE}\")\n",
    "print(Fore.GREEN, f\"Momentum set to : {GRADIENT_MOMENTUM}\")\n",
    "\n",
    "print(Fore.LIGHTMAGENTA_EX + \"Starting model training...\")\n",
    "train_time_start_on_gpu = timer()\n",
    "training_complete = False\n",
    "model_accuracy = train_neural_net(MAX_EPOCHS, selected_model, loss_func, optimizer, train_batches, val_batches)\n",
    "print(Fore.LIGHTCYAN_EX + f\"Training complete : {model_accuracy} %\")\n",
    "os.system(f'say \"Training complete\"')\n",
    "training_complete = True\n",
    "display_training_time(start=train_time_start_on_gpu,\n",
    "                  end=timer(),\n",
    "                  device=compute_unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3562f6a3",
   "metadata": {},
   "source": [
    "INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549c6ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_complete == True:\n",
    "    if input(\"Save model ? y for YES\") == \"y\":\n",
    "        print(Fore.LIGHTMAGENTA_EX + f\"Saving model at {SAVING_PATH}\")\n",
    "        torch.save(selected_model, SAVING_PATH)\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f5df7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECTOGRAM_SAVE_PATH = ''\n",
    "DEVICE = torch.device('cpu')\n",
    "def infer(sound_path : str) -> int:\n",
    "    model = torch.load(\"./model_path\", map_location=DEVICE)\n",
    "    sound = audio(sound_path)\n",
    "    sound.write_disk_specogram(SPECTOGRAM_SAVE_PATH, dpi = 90)\n",
    "    image = Image.open(SPECTOGRAM_SAVE_PATH).convert('RGB')\n",
    "    with torch.no_grad():\n",
    "        image_array = np.array(image)\n",
    "        image_array = np.transpose(image_array, (2,0,1))\n",
    "        image_tensor = torch.tensor(image_array, dtype=torch.float32).unsqueeze(0)\n",
    "        predictions = model(image_tensor)\n",
    "        top_index = torch.argmax(predictions, dim =1).item()\n",
    "    return predictions[top_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7b8cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECTOGRAM_DPI = 90\n",
    "DEFAULT_SAMPLE_RATE = 44100\n",
    "DEFAULT_HOPE_LWNGTH = 1024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
