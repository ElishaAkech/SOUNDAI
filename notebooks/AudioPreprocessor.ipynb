{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d3a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "from pathlib import Path\n",
    "import soundfile as sf\n",
    "import random\n",
    "\n",
    "class AudioPreprocessor:\n",
    "    def __init__(self, sample_rate=22050, segment_duration=2.0, n_mels=64):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.segment_duration = segment_duration\n",
    "        self.n_mels = n_mels\n",
    "        self.hop_length = 256\n",
    "        self.n_fft = 1024\n",
    "        self.max_segments = 5\n",
    "\n",
    "    @staticmethod\n",
    "    def load_audio(self, file_path):\n",
    "        \"\"\"Load audio file with librosa and resample if needed.\"\"\"\n",
    "        audio, sr = librosa.load(file_path, sr=self.sample_rate, mono=True)\n",
    "        return audio\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_segment(self, audio):\n",
    "        \"\"\"Extract random segments of fixed duration from audio.\"\"\"\n",
    "        target_length = int(self.segment_duration * self.sample_rate)\n",
    "        if len(audio) < target_length:\n",
    "            # Pad with zeros if too short\n",
    "            audio = np.pad(audio, (0, target_length - len(audio)), mode='constant')\n",
    "        else:\n",
    "            # Randomly select start point\n",
    "            max_start = len(audio) - target_length\n",
    "            start = random.randint(0, max_start)\n",
    "            audio = audio[start:start + target_length]\n",
    "        return audio\n",
    "    \n",
    "    @staticmethod\n",
    "    def augment_audio(self, audio):\n",
    "        \"\"\"Apply custom audio augmentation techniques.\"\"\"\n",
    "        # Random pitch shift\n",
    "        pitch_factor = random.uniform(-2, 2)\n",
    "        audio = librosa.effects.pitch_shift(audio, sr=self.sample_rate, n_steps=pitch_factor)\n",
    "        \n",
    "        # Add random noise\n",
    "        noise_level = random.uniform(0.001, 0.005)\n",
    "        noise = np.random.normal(0, noise_level, len(audio))\n",
    "        audio = audio + noise\n",
    "        \n",
    "        # Time stretch\n",
    "        stretch_factor = random.uniform(0.8, 1.2)\n",
    "        audio = librosa.effects.time_stretch(audio, rate=stretch_factor)\n",
    "        \n",
    "        return audio\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_features(self, audio):\n",
    "        \"\"\"Extract custom feature set combining mel spectrogram and spectral features.\"\"\"\n",
    "        # Mel spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=audio,\n",
    "            sr=self.sample_rate,\n",
    "            n_mels=self.n_mels,\n",
    "            n_fft=self.n_fft,\n",
    "            hop_length=self.hop_length\n",
    "        )\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "        # Spectral contrast\n",
    "        contrast = librosa.feature.spectral_contrast(\n",
    "            y=audio,\n",
    "            sr=self.sample_rate,\n",
    "            n_bands=6,\n",
    "            hop_length=self.hop_length\n",
    "        )\n",
    "\n",
    "        # Spectral rolloff\n",
    "        rolloff = librosa.feature.spectral_rolloff(\n",
    "            y=audio,\n",
    "            sr=self.sample_rate,\n",
    "            hop_length=self.hop_length\n",
    "        )\n",
    "\n",
    "        # Combine features\n",
    "        features = np.concatenate([\n",
    "            mel_spec_db.T,\n",
    "            contrast.T,\n",
    "            rolloff.T\n",
    "        ], axis=1)\n",
    "\n",
    "        return features\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_features(self, features):\n",
    "        \"\"\"Normalize features using robust scaling.\"\"\"\n",
    "        median = np.median(features, axis=0)\n",
    "        iqr = np.percentile(features, 75, axis=0) - np.percentile(features, 25, axis=0)\n",
    "        iqr = np.where(iqr == 0, 1, iqr)  # Avoid division by zero\n",
    "        normalized = (features - median) / iqr\n",
    "        return normalized\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_file(self, file_path):\n",
    "        \"\"\"Process a single audio file.\"\"\"\n",
    "        audio = self.load_audio(file_path)\n",
    "        segments = []\n",
    "        \n",
    "        for _ in range(self.max_segments):\n",
    "            segment = self.random_segment(audio)\n",
    "            segment = self.augment_audio(segment)\n",
    "            features = self.extract_features(segment)\n",
    "            normalized_features = self.normalize_features(features)\n",
    "            segments.append(normalized_features)\n",
    "        \n",
    "        return np.array(segments)\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_dataset(self, input_dir, output_dir):\n",
    "        \"\"\"Process all audio files in a directory.\"\"\"\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        for audio_file in Path(input_dir).glob(\"*.wav\"):\n",
    "            processed_data = self.process_file(audio_file)\n",
    "            output_path = output_dir / f\"processed_{audio_file.stem}.npy\"\n",
    "            np.save(output_path, processed_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocessor = AudioPreprocessor()\n",
    "    preprocessor.process_dataset(\n",
    "        input_dir=\"path/to/audio/files\",\n",
    "        output_dir=\"path/to/output/processed\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e997fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import random\n",
    "from torch.utils.data import Dataset,  random_split, DataLoader\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Updated SoundDS class\n",
    "class SoundDS(Dataset):\n",
    "    def __init__(self, df, data_path):\n",
    "        self.df = df\n",
    "        self.data_path = str(data_path)\n",
    "        self.preprocessor = AudioPreprocessor(sample_rate=44100, segment_duration=2.0, n_mels=64)\n",
    "        self.duration = 4000  # Original duration in ms, but we'll use segment_duration from preprocessor\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        relative_path = self.df.loc[idx, 'relative_path'].lstrip('/')\n",
    "        audio_file = os.path.join(self.data_path, 'audio', relative_path)\n",
    "        \n",
    "        if not os.path.exists(audio_file):\n",
    "            raise FileNotFoundError(f\"Audio file not found: {audio_file}\")\n",
    "        \n",
    "        class_id = self.df.loc[idx, 'classID']\n",
    "        \n",
    "        # Process audio using AudioPreprocessor\n",
    "        processed_segments = self.preprocessor.process_file(audio_file)\n",
    "        \n",
    "        # Convert to tensor (assuming PyTorch usage)\n",
    "        processed_segments = torch.from_numpy(processed_segments).float()\n",
    "        \n",
    "        return processed_segments, class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035aa897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the dataset\n",
    "def split_dataset(dataset, train_ratio=0.8, val_ratio=0.2):\n",
    "    assert train_ratio + val_ratio == 1.0, \"Ratios must sum to 1.0\"\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(train_ratio * total_size)\n",
    "    val_size = total_size - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a809f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders (optional, for batching)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    " # Example: Iterate over a batch\n",
    "for batch_segments, batch_labels in train_loader:\n",
    "    print(f\"Batch segments shape: {batch_segments.shape}, Batch labels shape: {batch_labels.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6f6cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EnhancedAudioCNN(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes, max_segments=5, feature_dim=128):\n",
    "        super(EnhancedAudioCNN, self).__init__()\n",
    "        \n",
    "        # Initial convolutional layer\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_channels, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        \n",
    "        # First residual block\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        \n",
    "        # Second residual block\n",
    "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        \n",
    "        # Depthwise separable block\n",
    "        self.depthwise_conv = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, groups=64, padding=1)\n",
    "        self.pointwise_conv = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=1)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.dropout4 = nn.Dropout(0.4)\n",
    "        init.kaiming_normal_(self.depthwise_conv.weight, a=0.1)\n",
    "        self.depthwise_conv.bias.data.zero_()\n",
    "        init.kaiming_normal_(self.pointwise_conv.weight, a=0.1)\n",
    "        self.pointwise_conv.bias.data.zero_()\n",
    "        \n",
    "        # Global average pooling\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(128 * max_segments, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, segments, time_steps, channels = x.size()\n",
    "        \n",
    "        # Reshape for convolution\n",
    "        x = x.view(batch_size * segments, channels, time_steps)\n",
    "        \n",
    "        # Convolutional layers with residuals\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        residual = x\n",
    "        x = F.relu(self.bn2(self.conv2(x)) + residual)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        residual = x\n",
    "        x = F.relu(self.bn3(self.conv3(x)) + residual)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = F.relu(self.bn4(self.pointwise_conv(self.depthwise_conv(x))))\n",
    "        x = self.dropout4(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.global_pool(x)  # [batch_size * segments, 128, 1]\n",
    "        x = x.view(batch_size, segments * 128)  # Flatten segments\n",
    "        \n",
    "        # Linear layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_channels = 128\n",
    "    num_classes = 10\n",
    "    max_segments = 5\n",
    "    feature_dim = 128\n",
    "    \n",
    "    model = EnhancedAudioCNN(input_channels=input_channels, num_classes=num_classes, max_segments=max_segments, feature_dim=feature_dim)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    sample_input = torch.randn(32, max_segments, 100, input_channels)\n",
    "    output = model(sample_input)\n",
    "    print(f\"Output shape: {output.shape}\")  # Should be [32, num_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc8fb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_model(model, train_dl, val_dl, num_epochs, device):\n",
    "    # Loss Function, Optimizer, and Scheduler\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # Add weight decay\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=0.001,\n",
    "        steps_per_epoch=int(len(train_dl)),\n",
    "        epochs=num_epochs,\n",
    "        anneal_strategy='cosine'  # Switch to cosine for smoother learning\n",
    "    )\n",
    "\n",
    "    # Best validation accuracy for early stopping\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_prediction = 0\n",
    "        total_prediction = 0\n",
    "\n",
    "        for i, data in enumerate(train_dl):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)  # Shape: [batch_size, num_classes]\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, prediction = torch.max(outputs, 1)\n",
    "            correct_prediction += (prediction == labels).sum().item()\n",
    "            total_prediction += prediction.shape[0]\n",
    "\n",
    "        # Training epoch metrics\n",
    "        train_loss = running_loss / len(train_dl)\n",
    "        train_acc = correct_prediction / total_prediction\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct_prediction = 0\n",
    "        val_total_prediction = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in val_dl:\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, prediction = torch.max(outputs, 1)\n",
    "                val_correct_prediction += (prediction == labels).sum().item()\n",
    "                val_total_prediction += prediction.shape[0]\n",
    "\n",
    "        # Validation epoch metrics\n",
    "        val_loss = val_loss / len(val_dl)\n",
    "        val_acc = val_correct_prediction / val_total_prediction\n",
    "\n",
    "        # Print epoch stats\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"New best model saved with validation accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "    print('Finished Training')\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming model, train_dl, val_dl, and device are defined\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = EnhancedAudioCNN(input_channels=128, num_classes=10, max_segments=5, feature_dim=128).to(device)\n",
    "    \n",
    "    # Load your datasets (from previous split_dataset function)\n",
    "    # train_dataset, val_dataset = split_dataset(full_dataset, train_ratio=0.8, val_ratio=0.2)\n",
    "    # train_dl = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    # val_dl = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    num_epochs = 20\n",
    "    trained_model = train_model(model, train_dl, val_dl, num_epochs, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
