{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec876b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relative_path</th>\n",
       "      <th>classID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/road/Around Arya School(1°16_32_ S 36°49_29_ ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/road/Around Arya School(1°16_32_ S 36°49_29_ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/road/Around Arya School(1°16_32_ S 36°49_29_ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/road/Around Arya School(1°16_32_ S 36°49_29_ ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/road/Around Arya School(1°16_32_ S 36°49_29_ ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       relative_path  classID\n",
       "0  /road/Around Arya School(1°16_32_ S 36°49_29_ ...        5\n",
       "1  /road/Around Arya School(1°16_32_ S 36°49_29_ ...        1\n",
       "2  /road/Around Arya School(1°16_32_ S 36°49_29_ ...        1\n",
       "3  /road/Around Arya School(1°16_32_ S 36°49_29_ ...        6\n",
       "4  /road/Around Arya School(1°16_32_ S 36°49_29_ ...        9"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "download_path  = Path('data\\\\DATA\\\\output_chunks')\n",
    "\n",
    "metadata_file = download_path/'metadata.csv'\n",
    "df = pd.read_csv(metadata_file)\n",
    "df.head()\n",
    "\n",
    "df['relative_path'] = '/road/' + df['road'].astype(str) + '/' + df['slice_file_name'].astype(str)\n",
    "\n",
    "df = df[['relative_path', 'classID']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "976e9081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['light truck' 'motorcycle' 'medium truck' 'PSV' 'other' 'pickup'\n",
      " 'private car' 'SUV' 'bus' 'bicycle' 'heavy truck']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(metadata_file)\n",
    "\n",
    "# Get distinct values from the 'class' column\n",
    "distinct_classes = df['class'].unique()\n",
    "\n",
    "\n",
    "# Display the result\n",
    "print(distinct_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "373392f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Music\\SonusAI\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "import random\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "class AudioUtil:\n",
    "    @staticmethod\n",
    "    def open(audio_file):\n",
    "        sig, sr = torchaudio.load(audio_file)\n",
    "        return (sig, sr)\n",
    "\n",
    "    @staticmethod\n",
    "    def resample(aud, newsr):\n",
    "        sig, sr = aud\n",
    "        if (sr == newsr):\n",
    "            return aud\n",
    "        num_channels = sig.shape[0]\n",
    "        resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n",
    "        if (num_channels > 1):\n",
    "            retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n",
    "            resig = torch.cat([resig, retwo])\n",
    "        return ((resig, newsr))\n",
    "\n",
    "    @staticmethod\n",
    "    def rechannel(aud, new_channel):\n",
    "        sig, sr = aud\n",
    "        if (sig.shape[0] == new_channel):\n",
    "            return aud\n",
    "        if (new_channel == 1):\n",
    "            resig = sig[:1, :]\n",
    "        else:\n",
    "            resig = torch.cat([sig, sig])\n",
    "        return ((resig, sr))\n",
    "\n",
    "    @staticmethod\n",
    "    def pad_trunc(aud, max_ms):\n",
    "        sig, sr = aud\n",
    "        num_rows, sig_len = sig.shape\n",
    "        max_len = sr//1000 * max_ms\n",
    "        if (sig_len > max_len):\n",
    "            sig = sig[:,:max_len]\n",
    "        elif (sig_len < max_len):\n",
    "            pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "            pad_end_len = max_len - sig_len - pad_begin_len\n",
    "            pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "            pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "            sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "        return (sig, sr)\n",
    "\n",
    "    @staticmethod\n",
    "    def time_shift(aud, shift_limit):\n",
    "        sig, sr = aud\n",
    "        _, sig_len = sig.shape\n",
    "        shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "        return (sig.roll(shift_amt), sr)\n",
    "\n",
    "    @staticmethod\n",
    "    def mel_spectrogram_feature(aud, n_mels=32, n_fft=2822, hop_length=1103, f_max=8000):\n",
    "        sig, sr = aud\n",
    "        mel_spec = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sr,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length,\n",
    "            n_mels=n_mels,\n",
    "            f_max=f_max,\n",
    "            window_fn=torch.hann_window\n",
    "        )(sig)\n",
    "        log_mel = transforms.AmplitudeToDB(top_db=80)(mel_spec)  # Log scale as in the paper\n",
    "        return log_mel  # Shape: [1, 64, time_steps]\n",
    "\n",
    "    @staticmethod\n",
    "    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2):\n",
    "        _, n_mfcc, n_steps = spec.shape\n",
    "        mask_value = spec.mean()\n",
    "        aug_spec = spec\n",
    "        freq_mask_param = max_mask_pct * n_mfcc\n",
    "        for _ in range(n_freq_masks):\n",
    "            aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "        time_mask_param = max_mask_pct * n_steps\n",
    "        for _ in range(n_time_masks):\n",
    "            aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "        return aug_spec\n",
    "    \n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def image_augment(spec):\n",
    "        spec_np = spec.squeeze(0).numpy()  # To [40, 512]\n",
    "        transform = A.Compose([\n",
    "            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=30, p=0.5),\n",
    "            A.GridDistortion(p=0.5),\n",
    "            A.Cutout(num_holes=8, max_h_size=8, max_w_size=8, p=0.5),\n",
    "        ])\n",
    "        augmented = transform(image=spec_np)['image']\n",
    "        aug_spec = torch.from_numpy(augmented).unsqueeze(0)  # Back to [1, 40, 512]\n",
    "        return aug_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f86e8ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "\n",
    "class SoundDS(Dataset):\n",
    "    def __init__(self, df, data_path, train=True):\n",
    "        self.df = df\n",
    "        self.data_path = str(data_path)\n",
    "        self.duration = 4000  # 4 seconds\n",
    "        self.sr = 44100      # Match paper\n",
    "        self.channel = 1\n",
    "        self.shift_pct = 0.4 if train else 0.0\n",
    "        self.train = train\n",
    "        self.target_time_steps = 512  # Adjust based on new hop_length if needed\n",
    "        self.hop_length = 1103       # Match paper\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Use 'relative_path' and remove the '/road/' prefix to match the directory structure\n",
    "        relative_path = self.df.loc[idx, 'relative_path']\n",
    "        adjusted_path = relative_path[6:]  # Remove '/road/' (length 6)\n",
    "        audio_file = os.path.join(self.data_path, adjusted_path)\n",
    "\n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(audio_file):\n",
    "            raise FileNotFoundError(f\"Audio file not found: {audio_file}\")\n",
    "\n",
    "        # Get the Class ID\n",
    "        class_id = self.df.loc[idx, 'classID']\n",
    "\n",
    "        # Load and process audio\n",
    "        aud = AudioUtil.open(audio_file)\n",
    "        reaud = AudioUtil.resample(aud, self.sr)  # Resample to 44100 Hz\n",
    "        rechan = AudioUtil.rechannel(reaud, self.channel)  # Force to 1 channel\n",
    "        dur_aud = AudioUtil.pad_trunc(rechan, self.duration)  # Pad or truncate to 4000 ms\n",
    "        shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct) if self.train else dur_aud\n",
    "\n",
    "        # Extract mel-spectrogram features with n_mels=32\n",
    "        mel_feat = AudioUtil.mel_spectrogram_feature(shift_aud, n_mels=32, n_fft=2822, hop_length=self.hop_length, f_max=8000)\n",
    "        # Ensure time dimension is exactly 512\n",
    "        _, n_mels, n_steps = mel_feat.shape\n",
    "        if n_steps > self.target_time_steps:\n",
    "            mel_feat = mel_feat[:, :, :self.target_time_steps]  # Truncate to 512\n",
    "        elif n_steps < self.target_time_steps:\n",
    "            pad_size = self.target_time_steps - n_steps\n",
    "            pad = torch.zeros((1, n_mels, pad_size))  # 1 channel\n",
    "            mel_feat = torch.cat((mel_feat, pad), dim=2)  # Pad with zeros\n",
    "\n",
    "        aug_mel = AudioUtil.spectro_augment(mel_feat, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2) if self.train else mel_feat\n",
    "\n",
    "        return aug_mel, class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7d5c44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class UrbanSoundModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(UrbanSoundModel, self).__init__()\n",
    "        \n",
    "        # Global Feature Extractor (reduced layers)\n",
    "        self.global_conv_blocks = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=(3, 3), padding=1),\n",
    "            nn.GroupNorm(4, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1),\n",
    "            nn.GroupNorm(8, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), padding=1),\n",
    "            nn.GroupNorm(16, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2))\n",
    "        )\n",
    "        self.global_rnn = nn.GRU(32, 128, bidirectional=True, batch_first=True)  # Updated input_size to 32\n",
    "\n",
    "        # Specific Feature Extractor (reduced layers)\n",
    "        self.specific_conv_blocks = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=(3, 3), padding=1),\n",
    "            nn.GroupNorm(4, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1),\n",
    "            nn.GroupNorm(8, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), padding=1),\n",
    "            nn.GroupNorm(16, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2))\n",
    "        )\n",
    "        self.specific_attention = nn.MultiheadAttention(32, num_heads=4, batch_first=True)  # embed_dim = 32\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 128),  # 128 (GRU) + 128 (Attention) = 256 (adjusted for symmetry)\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)  # Output logits\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, 1, 32, 512]\n",
    "        batch_size, channels, n_mels, time_steps = x.shape\n",
    "\n",
    "        # Global Feature Extraction\n",
    "        global_features = self.global_conv_blocks(x)  # [batch_size, 128, reduced_time]\n",
    "        global_features = global_features.squeeze(-1).squeeze(-1)  # [batch_size, 128]\n",
    "        # Reshape for GRU: remove channel dimension and transpose\n",
    "        global_features = x.squeeze(1).transpose(1, 2)  # [batch_size, 32, 512] -> [batch_size, 512, 32]\n",
    "        global_features, _ = self.global_rnn(global_features)  # [batch_size, 512, 256]\n",
    "        global_features = global_features[:, -1, :][:,:128]  # [batch_size, 128] (truncate to match)\n",
    "\n",
    "        # Specific Feature Extraction\n",
    "        specific_features = self.specific_conv_blocks(x)\n",
    "        specific_features = specific_features.squeeze(-1).squeeze(-1)  # [batch_size, 128]\n",
    "        specific_features = x.squeeze(1).transpose(1, 2)  # [batch_size, 32, 512] -> [batch_size, 512, 32]\n",
    "        specific_features, _ = self.specific_attention(specific_features, specific_features, specific_features)\n",
    "        specific_features = specific_features[:, -1, :]  # [batch_size, 32]\n",
    "\n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat((global_features, specific_features), dim=1)  # [batch_size, 160]\n",
    "        # Project to match expected input of fc layer\n",
    "        combined_features = nn.Linear(160, 256)(combined_features)  # Adjust to 256 for fc compatibility\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        output = self.fc(combined_features)  # Output logits [batch_size, 11]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02b34414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, train_dl, num_epochs, device):\n",
    "    print(\"Starting training loop...\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_dl):\n",
    "            inputs, labels = inputs.to(device), labels.to(device).long()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 10 == 9:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}], Loss: {running_loss/10:.4f}')\n",
    "                running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f783fb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Precision, Recall, F1Score, ConfusionMatrix\n",
    "import numpy as np\n",
    "# Inference/Evaluation Function\n",
    "def inference(model, val_dl, device):\n",
    "    model.eval()\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "    \n",
    "    precision = Precision(task=\"multilabel\", num_labels=model.fc[-2].out_features).to(device)\n",
    "    recall = Recall(task=\"multilabel\", num_labels=model.fc[-2].out_features).to(device)\n",
    "    f1 = F1Score(task=\"multilabel\", num_labels=model.fc[-2].out_features).to(device)\n",
    "    confmat = ConfusionMatrix(task=\"multilabel\", num_labels=model.fc[-2].out_features).to(device)\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in val_dl:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device).float()\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "\n",
    "            correct_prediction += (predicted == labels).sum().item()\n",
    "            total_prediction += labels.numel()\n",
    "            \n",
    "            precision.update(predicted, labels.int())\n",
    "            recall.update(predicted, labels.int())\n",
    "            f1.update(predicted, labels.int())\n",
    "            confmat.update(predicted, labels.int())\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(outputs.cpu().numpy())\n",
    "\n",
    "    acc = correct_prediction / total_prediction\n",
    "    prec = precision.compute().item()\n",
    "    rec = recall.compute().item()\n",
    "    f1_score = f1.compute().item()\n",
    "    conf_matrix = confmat.compute().cpu().numpy()\n",
    "\n",
    "    print(f'Accuracy: {acc:.4f}, Total items: {total_prediction}')\n",
    "    print(f'Precision: {prec:.4f}, Recall: {rec:.4f}, F1-Score: {f1_score:.4f}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'f1_score': f1_score,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'predictions': np.array(all_predictions),\n",
    "        'labels': np.array(all_labels),\n",
    "        'probabilities': np.array(all_probs)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c65663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Starting training loop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Music\\SonusAI\\venv\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [10], Loss: 2.3335\n",
      "Epoch [1/20], Step [20], Loss: 2.1816\n",
      "Epoch [1/20], Step [30], Loss: 2.2024\n",
      "Epoch [1/20], Step [40], Loss: 2.1510\n",
      "Epoch [1/20], Step [50], Loss: 2.0770\n",
      "Epoch [1/20], Step [60], Loss: 2.0152\n",
      "Epoch [1/20], Step [70], Loss: 2.1345\n",
      "Epoch [1/20], Step [80], Loss: 2.1147\n",
      "Epoch [1/20], Step [90], Loss: 1.9910\n",
      "Epoch [1/20], Step [100], Loss: 2.0688\n",
      "Epoch [1/20], Step [110], Loss: 2.0020\n",
      "Epoch [2/20], Step [10], Loss: 2.0695\n",
      "Epoch [2/20], Step [20], Loss: 2.0362\n",
      "Epoch [2/20], Step [30], Loss: 1.8904\n",
      "Epoch [2/20], Step [40], Loss: 2.0070\n",
      "Epoch [2/20], Step [50], Loss: 1.9745\n",
      "Epoch [2/20], Step [60], Loss: 2.0212\n",
      "Epoch [2/20], Step [70], Loss: 1.9123\n",
      "Epoch [2/20], Step [80], Loss: 1.9055\n",
      "Epoch [2/20], Step [90], Loss: 1.9509\n",
      "Epoch [2/20], Step [100], Loss: 1.9951\n",
      "Epoch [2/20], Step [110], Loss: 1.9549\n",
      "Epoch [3/20], Step [10], Loss: 1.9334\n",
      "Epoch [3/20], Step [20], Loss: 1.9567\n",
      "Epoch [3/20], Step [30], Loss: 2.0048\n",
      "Epoch [3/20], Step [40], Loss: 1.8670\n",
      "Epoch [3/20], Step [50], Loss: 1.8108\n",
      "Epoch [3/20], Step [60], Loss: 2.0256\n",
      "Epoch [3/20], Step [70], Loss: 2.0158\n",
      "Epoch [3/20], Step [80], Loss: 1.8598\n",
      "Epoch [3/20], Step [90], Loss: 2.0413\n",
      "Epoch [3/20], Step [100], Loss: 1.9718\n",
      "Epoch [3/20], Step [110], Loss: 1.9403\n",
      "Epoch [4/20], Step [10], Loss: 1.8651\n",
      "Epoch [4/20], Step [20], Loss: 2.0100\n",
      "Epoch [4/20], Step [30], Loss: 1.8532\n",
      "Epoch [4/20], Step [40], Loss: 2.0079\n",
      "Epoch [4/20], Step [50], Loss: 1.9594\n",
      "Epoch [4/20], Step [60], Loss: 1.9950\n",
      "Epoch [4/20], Step [70], Loss: 1.9191\n",
      "Epoch [4/20], Step [80], Loss: 1.9047\n",
      "Epoch [4/20], Step [90], Loss: 1.9230\n",
      "Epoch [4/20], Step [100], Loss: 2.0008\n",
      "Epoch [4/20], Step [110], Loss: 1.8154\n",
      "Epoch [5/20], Step [10], Loss: 1.8840\n",
      "Epoch [5/20], Step [20], Loss: 1.9306\n",
      "Epoch [5/20], Step [30], Loss: 2.0203\n",
      "Epoch [5/20], Step [40], Loss: 1.9391\n",
      "Epoch [5/20], Step [50], Loss: 1.8824\n",
      "Epoch [5/20], Step [60], Loss: 1.8961\n",
      "Epoch [5/20], Step [70], Loss: 2.0342\n",
      "Epoch [5/20], Step [80], Loss: 1.8816\n",
      "Epoch [5/20], Step [90], Loss: 1.9720\n",
      "Epoch [5/20], Step [100], Loss: 1.9766\n",
      "Epoch [5/20], Step [110], Loss: 1.9616\n",
      "Epoch [6/20], Step [10], Loss: 1.9846\n",
      "Epoch [6/20], Step [20], Loss: 1.9217\n",
      "Epoch [6/20], Step [30], Loss: 1.9507\n",
      "Epoch [6/20], Step [40], Loss: 1.8917\n",
      "Epoch [6/20], Step [50], Loss: 1.9542\n",
      "Epoch [6/20], Step [60], Loss: 1.9162\n",
      "Epoch [6/20], Step [70], Loss: 1.9846\n",
      "Epoch [6/20], Step [80], Loss: 1.8828\n",
      "Epoch [6/20], Step [90], Loss: 1.9568\n",
      "Epoch [6/20], Step [100], Loss: 1.8492\n",
      "Epoch [6/20], Step [110], Loss: 1.9408\n",
      "Epoch [7/20], Step [10], Loss: 1.9116\n",
      "Epoch [7/20], Step [20], Loss: 1.9991\n",
      "Epoch [7/20], Step [30], Loss: 1.9035\n",
      "Epoch [7/20], Step [40], Loss: 1.8438\n",
      "Epoch [7/20], Step [50], Loss: 1.8767\n",
      "Epoch [7/20], Step [60], Loss: 1.8948\n",
      "Epoch [7/20], Step [70], Loss: 1.9326\n",
      "Epoch [7/20], Step [80], Loss: 1.9065\n",
      "Epoch [7/20], Step [90], Loss: 1.8899\n",
      "Epoch [7/20], Step [100], Loss: 1.9580\n",
      "Epoch [7/20], Step [110], Loss: 1.9733\n",
      "Epoch [8/20], Step [10], Loss: 1.8583\n",
      "Epoch [8/20], Step [20], Loss: 1.9240\n",
      "Epoch [8/20], Step [30], Loss: 1.9402\n",
      "Epoch [8/20], Step [40], Loss: 1.9172\n",
      "Epoch [8/20], Step [50], Loss: 1.8654\n",
      "Epoch [8/20], Step [60], Loss: 2.0272\n",
      "Epoch [8/20], Step [70], Loss: 1.9211\n"
     ]
    }
   ],
   "source": [
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    download_path = Path('data\\\\DATA\\\\output_chunks')\n",
    "    metadata_file = download_path / 'metadata.csv'\n",
    "    df = pd.read_csv(metadata_file)\n",
    "    df['relative_path'] = '/road/' + df['road'].astype(str) + '/' + df['slice_file_name'].astype(str)\n",
    "    df = df[['relative_path', 'classID']]\n",
    "\n",
    "    # Dataset and DataLoader\n",
    "    dataset = SoundDS(df, download_path, train=True)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    train_dl = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)  # Reduced batch size for CPU\n",
    "    val_dl = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    # Device (forced to CPU)\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Model\n",
    "    model = UrbanSoundModel(num_classes=11).to(device)\n",
    "\n",
    "    # Training\n",
    "    num_epochs = 20  # Reduced for CPU efficiency\n",
    "    training(model, train_dl, num_epochs, device)\n",
    "\n",
    "    # Evaluation\n",
    "    results = inference(model, val_dl, device)\n",
    "\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), 'urban_sound_model_cpu.pth')\n",
    "    print(\"Model saved as 'urban_sound_model_cpu.pth'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
