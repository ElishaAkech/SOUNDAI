{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f2ccb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      "classID\n",
      "2     700\n",
      "9     405\n",
      "1     364\n",
      "5     212\n",
      "3     179\n",
      "6     130\n",
      "4      77\n",
      "10     68\n",
      "0      42\n",
      "7       6\n",
      "8       2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Prepare training data from Metadata file\n",
    "# ----------------------------\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "download_path = Path('data/DATA/output_chunks')\n",
    "\n",
    "# Read metadata file\n",
    "metadata_file = download_path / 'metadata.csv'\n",
    "df = pd.read_csv(metadata_file)\n",
    "df.head()\n",
    "\n",
    "# Construct file path by concatenating road and slice_file_name\n",
    "df['relative_path'] = '/road/' + df['road'].astype(str) + '/' + df['slice_file_name'].astype(str)\n",
    "\n",
    "# Take relevant columns\n",
    "df = df[['relative_path', 'classID']]\n",
    "df.head()\n",
    "\n",
    "# Check class balance\n",
    "class_counts = df['classID'].value_counts()\n",
    "print('Class distribution:')\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "515cc643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['relative_path', 'classID'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "869e4be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['light truck' 'motorcycle' 'medium truck' 'PSV' 'other' 'pickup'\n",
      " 'private car' 'SUV' 'bus' 'bicycle' 'heavy truck']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(metadata_file)\n",
    "\n",
    "# Get distinct values from the 'class' column\n",
    "distinct_classes = df['class'].unique()\n",
    "\n",
    "# Display the result\n",
    "print(distinct_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "016495b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#\n",
    "# Load the large CSV file\n",
    "#df = pd.read_csv(metadata_file)\n",
    "#\n",
    "# Define the mapping from class name to ClassID\n",
    "#class_to_id = {\n",
    "#    'bicycle': 0,\n",
    "#    'motorcycle': 1,\n",
    "#    'private car': 2,\n",
    "#    'SUV': 3,\n",
    "#    'pickup': 4,\n",
    "#    'light truck': 5,\n",
    "#    'medium truck': 6,\n",
    "#    'heavy truck': 7,\n",
    "#    'bus': 8,\n",
    "#    'PSV': 9,\n",
    "#    'other': 10\n",
    "#}\n",
    "#\n",
    "# Update the ClassID column based on the class column\n",
    "#df['classID'] = df['class'].map(class_to_id)\n",
    "#\n",
    "# Save the updated file (overwrite or create new)\n",
    "#df.to_csv(\"metadata_updated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50dbafe",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmath\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchaudio\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchaudio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transforms\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\SonusAI\\venv\\Lib\\site-packages\\torch\\__init__.py:2126\u001b[39m\n\u001b[32m   2119\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_compile\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _disable_dynamo  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[32m   2121\u001b[39m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[32m   2122\u001b[39m \u001b[38;5;66;03m# Import interface functions defined in Python\u001b[39;00m\n\u001b[32m   2123\u001b[39m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[32m   2124\u001b[39m \n\u001b[32m   2125\u001b[39m \u001b[38;5;66;03m# needs to be after the above ATen bindings so we can overwrite from Python side\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2126\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _VF \u001b[38;5;28;01mas\u001b[39;00m _VF, functional \u001b[38;5;28;01mas\u001b[39;00m functional  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[32m   2127\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# usort: skip # noqa: F403\u001b[39;00m\n\u001b[32m   2129\u001b[39m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[32m   2130\u001b[39m \u001b[38;5;66;03m# Remove unnecessary members\u001b[39;00m\n\u001b[32m   2131\u001b[39m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\SonusAI\\venv\\Lib\\site-packages\\torch\\functional.py:8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Optional, TYPE_CHECKING, Union\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _VF, Tensor\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _add_docstr\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\SonusAI\\venv\\Lib\\site-packages\\torch\\nn\\__init__.py:8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# mypy: allow-untyped-defs\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparameter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[32m      3\u001b[39m     Buffer \u001b[38;5;28;01mas\u001b[39;00m Buffer,\n\u001b[32m      4\u001b[39m     Parameter \u001b[38;5;28;01mas\u001b[39;00m Parameter,\n\u001b[32m      5\u001b[39m     UninitializedBuffer \u001b[38;5;28;01mas\u001b[39;00m UninitializedBuffer,\n\u001b[32m      6\u001b[39m     UninitializedParameter \u001b[38;5;28;01mas\u001b[39;00m UninitializedParameter,\n\u001b[32m      7\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodules\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# usort: skip # noqa: F403\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     10\u001b[39m     attention \u001b[38;5;28;01mas\u001b[39;00m attention,\n\u001b[32m     11\u001b[39m     functional \u001b[38;5;28;01mas\u001b[39;00m functional,\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     utils \u001b[38;5;28;01mas\u001b[39;00m utils,\n\u001b[32m     17\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparallel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataParallel \u001b[38;5;28;01mas\u001b[39;00m DataParallel\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\SonusAI\\venv\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodule\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Module  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinear\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bilinear, Identity, LazyLinear, Linear  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      4\u001b[39m     CELU,\n\u001b[32m      5\u001b[39m     ELU,\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     Threshold,\n\u001b[32m     33\u001b[39m )\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01madaptive\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AdaptiveLogSoftmaxWithLoss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\SonusAI\\venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F, init\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparameter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Parameter, UninitializedParameter\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlazy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LazyModuleMixin\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\SonusAI\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _VF, sym_int \u001b[38;5;28;01mas\u001b[39;00m _sym_int, Tensor\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _add_docstr, _infer_size\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_jit_internal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     12\u001b[39m     _overload,\n\u001b[32m     13\u001b[39m     boolean_dispatch,\n\u001b[32m     14\u001b[39m     BroadcastingList1,\n\u001b[32m     15\u001b[39m     BroadcastingList2,\n\u001b[32m     16\u001b[39m     BroadcastingList3,\n\u001b[32m     17\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_torch_docs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m reproducibility_notes, sparse_support_notes, tf32_notes\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _reduction \u001b[38;5;28;01mas\u001b[39;00m _Reduction, grad  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\SonusAI\\venv\\Lib\\site-packages\\torch\\_jit_internal.py:44\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrpc\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpackage\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_mangling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpackage_mangling\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_awaits\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _Await\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _Await \u001b[38;5;28;01mas\u001b[39;00m CAwait, Future \u001b[38;5;28;01mas\u001b[39;00m CFuture\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sources\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fake_range, get_source_lines_and_file, parse_def\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1322\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1262\u001b[39m, in \u001b[36m_find_spec\u001b[39m\u001b[34m(name, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1528\u001b[39m, in \u001b[36mfind_spec\u001b[39m\u001b[34m(cls, fullname, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1502\u001b[39m, in \u001b[36m_get_spec\u001b[39m\u001b[34m(cls, fullname, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1620\u001b[39m, in \u001b[36mfind_spec\u001b[39m\u001b[34m(self, fullname, target)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from IPython.display import Audio\n",
    "\n",
    "class AudioUtil:\n",
    "    @staticmethod\n",
    "    def open(audio_file):\n",
    "        sig, sr = torchaudio.load(audio_file)\n",
    "        return (sig, sr)\n",
    "\n",
    "    @staticmethod\n",
    "    def rechannel(aud, new_channel=1):\n",
    "        sig, sr = aud\n",
    "        if (sig.shape[0] == new_channel):\n",
    "            return aud\n",
    "        if (new_channel == 1):\n",
    "            if sig.shape[0] > 1:\n",
    "                resig = sig.mean(dim=0, keepdim=True)\n",
    "            else:\n",
    "                resig = sig\n",
    "        else:\n",
    "            resig = torch.cat([sig, sig])\n",
    "        return (resig, sr)\n",
    "\n",
    "    @staticmethod\n",
    "    def resample(aud, newsr):\n",
    "        sig, sr = aud\n",
    "        if (sr == newsr):\n",
    "            return aud\n",
    "        num_channels = sig.shape[0]\n",
    "        resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n",
    "        if (num_channels > 1):\n",
    "            retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n",
    "            resig = torch.cat([resig, retwo])\n",
    "        return ((resig, newsr))\n",
    "\n",
    "    @staticmethod\n",
    "    def pad_trunc(aud, max_ms):\n",
    "        sig, sr = aud\n",
    "        num_rows, sig_len = sig.shape\n",
    "        max_len = sr // 1000 * max_ms  # e.g., 132300 for 6s at 22kHz\n",
    "        if (sig_len > max_len):\n",
    "            sig = sig[:, :max_len]\n",
    "        elif (sig_len < max_len):\n",
    "            pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "            pad_end_len = max_len - sig_len - pad_begin_len\n",
    "            pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "            pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "            sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "        return (sig, sr)\n",
    "\n",
    "    @staticmethod\n",
    "    def time_shift(aud, shift_limit):\n",
    "        sig, sr = aud\n",
    "        _, sig_len = sig.shape\n",
    "        shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "        return (sig.roll(shift_amt), sr)\n",
    "\n",
    "    @staticmethod\n",
    "    def spectro_gram(aud, n_mels=128, n_fft=1024, hop_len=512):\n",
    "        sig, sr = aud\n",
    "        if sig.shape[0] != 1:\n",
    "            raise ValueError(f\"Expected mono audio, got {sig.shape[0]} channels\")\n",
    "        top_db = 80\n",
    "        spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)  # Shape: [1, 128, 259]\n",
    "        # Compute deltas\n",
    "        delta = torchaudio.functional.compute_deltas(spec)  # Shape: [1, 128, 259]\n",
    "        delta_delta = torchaudio.functional.compute_deltas(delta)  # Shape: [1, 128, 259]\n",
    "        # Squeeze channel dimension and stack\n",
    "        spec = spec.squeeze(0)  # Shape: [128, 259]\n",
    "        delta = delta.squeeze(0)  # Shape: [128, 259]\n",
    "        delta_delta = delta_delta.squeeze(0)  # Shape: [128, 259]\n",
    "        spec = torch.stack([spec, delta, delta_delta], dim=0)  # Shape: [3, 128, 259]\n",
    "        # Per-sample normalization\n",
    "        spec = (spec - spec.mean()) / (spec.std() + 1e-6)\n",
    "        # Ensure fixed time dimension (259 for 6s at 22kHz, hop_len=512)\n",
    "        expected_time_steps = 259\n",
    "        if spec.shape[-1] != expected_time_steps:\n",
    "            if spec.shape[-1] > expected_time_steps:\n",
    "                spec = spec[:, :, :expected_time_steps]\n",
    "            else:\n",
    "                pad_len = expected_time_steps - spec.shape[-1]\n",
    "                spec = torch.nn.functional.pad(spec, (0, pad_len))\n",
    "        return spec\n",
    "\n",
    "    @staticmethod\n",
    "    def spectro_augment(spec, max_mask_pct=0.4, n_freq_masks=4, n_time_masks=4):\n",
    "        _, n_mels, n_steps = spec.shape\n",
    "        mask_value = spec.mean()\n",
    "        aug_spec = spec\n",
    "        freq_mask_param = max_mask_pct * n_mels\n",
    "        for _ in range(n_freq_masks):\n",
    "            aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "        time_mask_param = max_mask_pct * n_steps\n",
    "        for _ in range(n_time_masks):\n",
    "            aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "        return aug_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23455bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SoundDS(Dataset):\n",
    "    def __init__(self, df, data_path):\n",
    "        self.df = df\n",
    "        self.data_path = str(data_path)\n",
    "        self.duration = 6000  # 6s\n",
    "        self.sr = 22050  # 22kHz\n",
    "        self.channel = 1  # Mono\n",
    "        self.shift_pct = 0.4\n",
    "        self.aug_prob = 0.5\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Construct the full path\n",
    "        road = self.df.loc[idx, 'road']\n",
    "        slice_file_name = self.df.loc[idx, 'slice_file_name']\n",
    "        relative_path = road + '/' + slice_file_name\n",
    "        audio_file = os.path.join(self.data_path, relative_path)\n",
    "        \n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(audio_file):\n",
    "            raise FileNotFoundError(f\"Audio file not found: {audio_file}\")\n",
    "        \n",
    "        # Get the Class ID\n",
    "        class_id = self.df.loc[idx, 'classID']\n",
    "\n",
    "        aud = AudioUtil.open(audio_file)\n",
    "        reaud = AudioUtil.resample(aud, self.sr)\n",
    "        rechan = AudioUtil.rechannel(reaud, self.channel)\n",
    "        \n",
    "        # Pad/truncate first to ensure 6s length\n",
    "        dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n",
    "        \n",
    "        # Apply augmentations\n",
    "        if random.random() < self.aug_prob:\n",
    "            # Time shift\n",
    "            shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n",
    "            # Gaussian noise\n",
    "            noise = torch.randn_like(shift_aud[0]) * random.uniform(0.001, 0.015)\n",
    "            shift_aud = (shift_aud[0] + noise, shift_aud[1])\n",
    "        else:\n",
    "            shift_aud = dur_aud\n",
    "\n",
    "        sgram = AudioUtil.spectro_gram(shift_aud, n_mels=128, n_fft=1024, hop_len=512)\n",
    "        aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.4, n_freq_masks=4, n_time_masks=4)\n",
    "\n",
    "        # Validate shape\n",
    "        expected_shape = (3, 128, 259)  # [feature_channels, mels, time_steps] for 6s\n",
    "        if aug_sgram.shape != expected_shape:\n",
    "            raise ValueError(f\"Unexpected spectrogram shape {aug_sgram.shape} at index {idx}, file: {audio_file}. Expected {expected_shape}\")\n",
    "\n",
    "        return aug_sgram, class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4894d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Separate spectrograms and labels\n",
    "    spectrograms, labels = zip(*batch)\n",
    "    # Convert to tensors and stack\n",
    "    spectrograms = torch.stack(spectrograms, dim=0)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return spectrograms, labels\n",
    "\n",
    "myds = SoundDS(df, download_path)\n",
    "\n",
    "# Random split of 80:20 between training and validation\n",
    "num_items = len(myds)\n",
    "num_train = round(num_items * 0.8)\n",
    "num_val = num_items - num_train\n",
    "train_ds, val_ds = random_split(myds, [num_train, num_val])\n",
    "\n",
    "# Create training and validation data loaders with custom collate\n",
    "train_dl = DataLoader(train_ds, batch_size=16, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_dl = DataLoader(val_ds, batch_size=16, shuffle=False, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95e4166",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#import torchaudio\n",
    "\n",
    "# Debug dataset shapes and audio lengths\n",
    "#for i in range(min(10, len(myds))):\n",
    "#    try:\n",
    "#        # Get file path\n",
    "#        road = myds.df.loc[i, 'road']\n",
    "#        slice_file_name = myds.df.loc[i, 'slice_file_name']\n",
    "#        relative_path = road + '/' + slice_file_name\n",
    "#        audio_file = os.path.join(myds.data_path, relative_path)\n",
    "#        \n",
    "#        # Load raw audio to check length and channels\n",
    "#        aud = AudioUtil.open(audio_file)\n",
    "#        raw_len = aud[0].shape[-1]\n",
    "#        raw_channels = aud[0].shape[0]\n",
    "#        raw_duration = raw_len / aud[1]  # Duration in seconds\n",
    "#       \n",
    "#        # Process audio\n",
    "#        reaud = AudioUtil.resample(aud, myds.sr)\n",
    "#        rechan = AudioUtil.rechannel(reaud, myds.channel)\n",
    "#        dur_aud = AudioUtil.pad_trunc(rechan, myds.duration)\n",
    "#       \n",
    "#        # Compute intermediate spectrograms\n",
    "#       spec = torchaudio.transforms.MelSpectrogram(myds.sr, n_fft=1024, hop_length=512, n_mels=128)(dur_aud[0])\n",
    "#        spec = torchaudio.transforms.AmplitudeToDB(top_db=80)(spec)\n",
    "#       delta = torchaudio.functional.compute_deltas(spec)\n",
    "#        \n",
    "#        # Get final spectrogram\n",
    "#        sgram, class_id = myds[i]\n",
    "#        \n",
    "#        print(f\"Index {i}: File {audio_file}, Raw length {raw_len} samples ({raw_duration:.2f}s), Raw channels {raw_channels}, Processed channels {dur_aud[0].shape[0]}, Mel shape {spec.shape}, Delta shape {delta.shape}, Final shape {sgram.shape}, Class ID {class_id}\")\n",
    "#    except Exception as e:\n",
    "#        print(f\"Error at index {i}, file {audio_file}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc931e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "# ----------------------------\n",
    "# Audio Classification Model\n",
    "# ----------------------------\n",
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=11):\n",
    "        super().__init__()\n",
    "        # Load pretrained ResNet18\n",
    "        self.resnet = resnet18(pretrained=True)\n",
    "        # Modify first conv layer for 3-channel input (mel + delta + delta-delta)\n",
    "        self.resnet.conv1 = nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        # Modify final fully connected layer for 11 classes\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Dropout(0.3),  # Add dropout\n",
    "            nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "        )\n",
    "        # Freeze early layers\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Unfreeze later layers\n",
    "        for param in self.resnet.layer4.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.resnet.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "\n",
    "# Create the model and put it on the GPU if available\n",
    "myModel = AudioClassifier()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "myModel = myModel.to(device)\n",
    "# Check that it is on Cuda\n",
    "next(myModel.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7138a222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchmetrics import Precision, Recall, F1Score, ConfusionMatrix\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def inference(model, data_dl, device):\n",
    "    model.eval()\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "    \n",
    "    # Initialize metrics\n",
    "    precision = Precision(task=\"multiclass\", num_classes=11).to(device)\n",
    "    recall = Recall(task=\"multiclass\", num_classes=11).to(device)\n",
    "    f1 = F1Score(task=\"multiclass\", num_classes=11).to(device)\n",
    "    confmat = ConfusionMatrix(task=\"multiclass\", num_classes=11).to(device)\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_dl:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # Get predictions\n",
    "            outputs = model(inputs)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "            # Update metrics\n",
    "            correct_prediction += (predictions == labels).sum().item()\n",
    "            total_prediction += predictions.shape[0]\n",
    "            \n",
    "            precision.update(predictions, labels)\n",
    "            recall.update(predictions, labels)\n",
    "            f1.update(predictions, labels)\n",
    "            confmat.update(predictions, labels)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    acc = correct_prediction / total_prediction\n",
    "    prec = precision.compute().item()\n",
    "    rec = recall.compute().item()\n",
    "    f1_score = f1.compute().item()\n",
    "    conf_matrix = confmat.compute().cpu().numpy()\n",
    "\n",
    "    # Print results\n",
    "    print(f'Accuracy: {acc:.4f}, Total items: {total_prediction}')\n",
    "    print(f'Precision: {prec:.4f}, Recall: {rec:.4f}, F1-Score: {f1_score:.4f}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "    print('Classification Report:')\n",
    "    print(classification_report(all_labels, all_predictions, digits=4))\n",
    "\n",
    "    # Return results for further analysis\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'f1_score': f1_score,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        #'predictions': np.array(all_predictions),\n",
    "        #'labels': np.array(all_labels),\n",
    "        #'probabilities': np.array(all_probs)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2c86d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# Training Loop\n",
    "# ----------------------------\n",
    "def training(model, train_dl, val_dl, num_epochs, patience=20):\n",
    "    # Compute class weights for balanced loss\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(df['classID']), y=df['classID'])\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "    # Loss Function, Optimizer and Scheduler\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4, nesterov=True)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[40, 80], gamma=0.1)\n",
    "\n",
    "    best_val_acc = 0\n",
    "    counter = 0\n",
    "\n",
    "    # Repeat for each epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_prediction = 0\n",
    "        total_prediction = 0\n",
    "\n",
    "        # Repeat for each batch in the training set\n",
    "        for i, data in enumerate(train_dl):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Keep stats for Loss and Accuracy\n",
    "            running_loss += loss.item()\n",
    "            _, prediction = torch.max(outputs, 1)\n",
    "            correct_prediction += (prediction == labels).sum().item()\n",
    "            total_prediction += prediction.shape[0]\n",
    "\n",
    "        # Print stats at the end of the epoch\n",
    "        num_batches = len(train_dl)\n",
    "        avg_loss = running_loss / num_batches\n",
    "        acc = correct_prediction / total_prediction\n",
    "        print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Training Accuracy: {acc:.2f}')\n",
    "\n",
    "        # Validation step (compute accuracy only, no printing)\n",
    "        model.eval()\n",
    "        correct_prediction = 0\n",
    "        total_prediction = 0\n",
    "        with torch.no_grad():\n",
    "            for data in val_dl:\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, prediction = torch.max(outputs, 1)\n",
    "                correct_prediction += (prediction == labels).sum().item()\n",
    "                total_prediction += prediction.shape[0]\n",
    "        val_acc = correct_prediction / total_prediction\n",
    "        print(f'Validation Accuracy: {val_acc:.2f}')\n",
    "\n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(\"Saved best model\")\n",
    "        else:\n",
    "            counter += 1\n",
    "        #if counter >= patience:\n",
    "        #    print(f'Early stopping at epoch {epoch}')\n",
    "        #    break\n",
    "\n",
    "    # Final validation step with full metrics\n",
    "    print('Finished Training')\n",
    "    final_results = inference(model, val_dl, device)\n",
    "    print(f\"Final Validation Metrics - Accuracy: {final_results['accuracy']:.4f}, \"\n",
    "          f\"Total items: {final_results['total_items']}, \"\n",
    "          f\"Precision: {final_results['precision']:.4f}, \"\n",
    "          f\"Recall: {final_results['recall']:.4f}, \"\n",
    "          f\"F1-Score: {final_results['f1']:.4f}, \"\n",
    "          f\"Confusion Matrix:\\n{final_results['confusion_matrix']}\")\n",
    "    \n",
    "    return best_val_acc\n",
    "\n",
    "num_epochs = 200\n",
    "best_acc = training(myModel, train_dl, val_dl, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df70ef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = AudioClassifier(num_classes=11).to(device)\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    results = inference(model, val_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0529629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(myModel.state_dict(), 'audio_classifier_model.pth')\n",
    "print(\"Model saved as 'audio_classifier_model.pth'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
